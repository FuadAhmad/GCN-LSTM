{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868cda37-52bb-4b60-8028-c5de3c82aca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 125 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Keras\n",
      "Successfully installed Keras-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras #Keras-2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc1b178-13d9-4a0c-87cb-948394c50945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp39-cp39-macosx_10_14_x86_64.whl (217.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.5 MB 39 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorflow) (1.21.2)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorflow) (3.11.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorflow) (1.13.3)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-macosx_10_9_x86_64.whl (13.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.0 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp39-cp39-macosx_10_10_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 2.5 MB/s eta 0:00:01     |███████▌                        | 993 kB 2.2 MB/s eta 0:00:02     |███████████████████▍            | 2.6 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/fuad/opt/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=e484e671e30fe4341b860fecb07a90e3363ea69f2dfe65e5d0f7db72b2fc5104\n",
      "  Stored in directory: /Users/fuad/Library/Caches/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-3.6.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 werkzeug-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2cd9a5-79ef-4858-b67f-c46f3dd4a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd30683-c294-49ab-92e6-eb19005b85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading option 2: from local file\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load(file_name):\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        obj = pickle.load(fp)\n",
    "    return obj\n",
    "\n",
    "path = os.getcwd() #'/Users/fuad/Documents/GitHub/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5907e833-8cde-4b90-aa63-9bd73805e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project folder location where the models and data folder exist\n",
    "projectlocation = '/Users/fuad/Documents/GitHub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "989e4b41-3f9f-4247-9560-01b2954757c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData.shape:  (1540, 33, 60)\n",
      "trainLebel.shape:  (1540,)\n"
     ]
    }
   ],
   "source": [
    "#set data path and load data\n",
    "datapath = projectlocation + \"/data/flare_prediction_mvts_data.pck\"\n",
    "labelpath = projectlocation + \"/data/flare_prediction_labels.pck\"\n",
    "\n",
    "trainData = load(datapath)\n",
    "trainLebel = load(labelpath)\n",
    "\n",
    "print(\"trainData.shape: \",trainData.shape)\n",
    "print(\"trainLebel.shape: \",trainLebel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef77c38-933f-418c-bd61-f358b6893db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.unique(trainLebel):  [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "#four-class problem {X, M, B/C, Q}\n",
    "print(\"np.unique(trainLebel): \",np.unique(trainLebel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a864094b-d971-45b5-b74b-f7b85be3b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary classification\n",
    "# Data label conversion to BINARY class\n",
    "def get_binary_labels_from(labels_str):\n",
    "    tdf = pd.DataFrame(labels_str, columns = ['labels'])\n",
    "    data_classes= [0, 1, 2, 3]\n",
    "    d = dict(zip(data_classes, [0, 0, 1, 1])) \n",
    "    arr = tdf['labels'].map(d, na_action='ignore')\n",
    "    return arr.to_numpy()\n",
    "\n",
    "#un-comment next line for Binary classification experiment\n",
    "trainLebel = get_binary_labels_from(trainLebel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7171797-4733-48a5-adb7-4db7f20ba080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData.shape:  (1540, 60, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6748.177901</td>\n",
       "      <td>8.294421e+10</td>\n",
       "      <td>1.591225e+24</td>\n",
       "      <td>1.095967e+14</td>\n",
       "      <td>1510.190735</td>\n",
       "      <td>5.537032e+13</td>\n",
       "      <td>6.524454e+22</td>\n",
       "      <td>-1.945966e+25</td>\n",
       "      <td>13461.392092</td>\n",
       "      <td>-0.176659</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414326</td>\n",
       "      <td>5.392323</td>\n",
       "      <td>5.336158</td>\n",
       "      <td>5.205328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1424.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6713.245495</td>\n",
       "      <td>8.301113e+10</td>\n",
       "      <td>1.591180e+24</td>\n",
       "      <td>1.082126e+14</td>\n",
       "      <td>1552.899870</td>\n",
       "      <td>5.163912e+13</td>\n",
       "      <td>6.552311e+22</td>\n",
       "      <td>-1.967367e+25</td>\n",
       "      <td>13459.043822</td>\n",
       "      <td>-0.178458</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405846</td>\n",
       "      <td>5.403088</td>\n",
       "      <td>5.335880</td>\n",
       "      <td>5.211822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6711.746592</td>\n",
       "      <td>8.324237e+10</td>\n",
       "      <td>1.585508e+24</td>\n",
       "      <td>1.074454e+14</td>\n",
       "      <td>1569.593570</td>\n",
       "      <td>5.208053e+13</td>\n",
       "      <td>6.582194e+22</td>\n",
       "      <td>-2.022283e+25</td>\n",
       "      <td>13391.359484</td>\n",
       "      <td>-0.182930</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405772</td>\n",
       "      <td>5.398195</td>\n",
       "      <td>5.345981</td>\n",
       "      <td>5.216945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6903.974288</td>\n",
       "      <td>8.343257e+10</td>\n",
       "      <td>1.576556e+24</td>\n",
       "      <td>1.084795e+14</td>\n",
       "      <td>1608.218901</td>\n",
       "      <td>5.166028e+13</td>\n",
       "      <td>6.609184e+22</td>\n",
       "      <td>-2.072753e+25</td>\n",
       "      <td>13248.436229</td>\n",
       "      <td>-0.187068</td>\n",
       "      <td>...</td>\n",
       "      <td>5.396873</td>\n",
       "      <td>5.398732</td>\n",
       "      <td>5.327560</td>\n",
       "      <td>5.203755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1429.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6691.365852</td>\n",
       "      <td>8.370511e+10</td>\n",
       "      <td>1.600757e+24</td>\n",
       "      <td>1.086643e+14</td>\n",
       "      <td>1479.272564</td>\n",
       "      <td>5.030951e+13</td>\n",
       "      <td>6.614713e+22</td>\n",
       "      <td>-1.999304e+25</td>\n",
       "      <td>13367.513454</td>\n",
       "      <td>-0.179852</td>\n",
       "      <td>...</td>\n",
       "      <td>5.426061</td>\n",
       "      <td>5.389834</td>\n",
       "      <td>5.351214</td>\n",
       "      <td>5.217820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6834.958217</td>\n",
       "      <td>8.406158e+10</td>\n",
       "      <td>1.591036e+24</td>\n",
       "      <td>1.096684e+14</td>\n",
       "      <td>1644.488148</td>\n",
       "      <td>5.663543e+13</td>\n",
       "      <td>6.630166e+22</td>\n",
       "      <td>-2.075205e+25</td>\n",
       "      <td>13298.131406</td>\n",
       "      <td>-0.185888</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421926</td>\n",
       "      <td>5.407266</td>\n",
       "      <td>5.350388</td>\n",
       "      <td>5.219855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6830.348029</td>\n",
       "      <td>8.431275e+10</td>\n",
       "      <td>1.589851e+24</td>\n",
       "      <td>1.083214e+14</td>\n",
       "      <td>1587.280389</td>\n",
       "      <td>5.648788e+13</td>\n",
       "      <td>6.649235e+22</td>\n",
       "      <td>-2.105215e+25</td>\n",
       "      <td>13390.341243</td>\n",
       "      <td>-0.188014</td>\n",
       "      <td>...</td>\n",
       "      <td>5.419279</td>\n",
       "      <td>5.413495</td>\n",
       "      <td>5.353073</td>\n",
       "      <td>5.232006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1477.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6894.216660</td>\n",
       "      <td>8.429531e+10</td>\n",
       "      <td>1.577024e+24</td>\n",
       "      <td>1.090471e+14</td>\n",
       "      <td>1530.971594</td>\n",
       "      <td>5.153441e+13</td>\n",
       "      <td>6.685179e+22</td>\n",
       "      <td>-2.175302e+25</td>\n",
       "      <td>13251.479423</td>\n",
       "      <td>-0.194314</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413199</td>\n",
       "      <td>5.418618</td>\n",
       "      <td>5.341884</td>\n",
       "      <td>5.221831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6906.079329</td>\n",
       "      <td>8.449927e+10</td>\n",
       "      <td>1.586829e+24</td>\n",
       "      <td>1.088327e+14</td>\n",
       "      <td>1580.642155</td>\n",
       "      <td>5.204076e+13</td>\n",
       "      <td>6.646028e+22</td>\n",
       "      <td>-2.155312e+25</td>\n",
       "      <td>13485.254521</td>\n",
       "      <td>-0.192063</td>\n",
       "      <td>...</td>\n",
       "      <td>5.419249</td>\n",
       "      <td>5.426273</td>\n",
       "      <td>5.354147</td>\n",
       "      <td>5.243087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6899.094797</td>\n",
       "      <td>8.451908e+10</td>\n",
       "      <td>1.601676e+24</td>\n",
       "      <td>1.091217e+14</td>\n",
       "      <td>1562.139216</td>\n",
       "      <td>5.105149e+13</td>\n",
       "      <td>6.593766e+22</td>\n",
       "      <td>-2.084657e+25</td>\n",
       "      <td>13729.350076</td>\n",
       "      <td>-0.185724</td>\n",
       "      <td>...</td>\n",
       "      <td>5.427837</td>\n",
       "      <td>5.393532</td>\n",
       "      <td>5.321879</td>\n",
       "      <td>5.221579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6911.805621</td>\n",
       "      <td>8.453654e+10</td>\n",
       "      <td>1.584634e+24</td>\n",
       "      <td>1.085458e+14</td>\n",
       "      <td>1608.232234</td>\n",
       "      <td>5.032479e+13</td>\n",
       "      <td>6.626648e+22</td>\n",
       "      <td>-2.168199e+25</td>\n",
       "      <td>13676.366262</td>\n",
       "      <td>-0.193127</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421960</td>\n",
       "      <td>5.419106</td>\n",
       "      <td>5.349870</td>\n",
       "      <td>5.231510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1865.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6946.807156</td>\n",
       "      <td>8.448848e+10</td>\n",
       "      <td>1.588865e+24</td>\n",
       "      <td>1.085662e+14</td>\n",
       "      <td>1598.537641</td>\n",
       "      <td>4.720406e+13</td>\n",
       "      <td>6.604935e+22</td>\n",
       "      <td>-2.160836e+25</td>\n",
       "      <td>13798.923015</td>\n",
       "      <td>-0.192580</td>\n",
       "      <td>...</td>\n",
       "      <td>5.409650</td>\n",
       "      <td>5.423591</td>\n",
       "      <td>5.336414</td>\n",
       "      <td>5.216673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6886.818627</td>\n",
       "      <td>8.431700e+10</td>\n",
       "      <td>1.578638e+24</td>\n",
       "      <td>1.071168e+14</td>\n",
       "      <td>1581.500858</td>\n",
       "      <td>4.759987e+13</td>\n",
       "      <td>6.570224e+22</td>\n",
       "      <td>-2.183161e+25</td>\n",
       "      <td>13867.168557</td>\n",
       "      <td>-0.194966</td>\n",
       "      <td>...</td>\n",
       "      <td>5.404884</td>\n",
       "      <td>5.435943</td>\n",
       "      <td>5.344661</td>\n",
       "      <td>5.215722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1874.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6818.410837</td>\n",
       "      <td>8.425498e+10</td>\n",
       "      <td>1.577545e+24</td>\n",
       "      <td>1.059483e+14</td>\n",
       "      <td>1552.157851</td>\n",
       "      <td>4.509200e+13</td>\n",
       "      <td>6.554720e+22</td>\n",
       "      <td>-2.179559e+25</td>\n",
       "      <td>14013.525138</td>\n",
       "      <td>-0.194787</td>\n",
       "      <td>...</td>\n",
       "      <td>5.395443</td>\n",
       "      <td>5.426074</td>\n",
       "      <td>5.338376</td>\n",
       "      <td>5.218632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6837.526272</td>\n",
       "      <td>8.424089e+10</td>\n",
       "      <td>1.577035e+24</td>\n",
       "      <td>1.051378e+14</td>\n",
       "      <td>1623.889248</td>\n",
       "      <td>4.995272e+13</td>\n",
       "      <td>6.540712e+22</td>\n",
       "      <td>-2.187353e+25</td>\n",
       "      <td>14092.449955</td>\n",
       "      <td>-0.195516</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414608</td>\n",
       "      <td>5.430777</td>\n",
       "      <td>5.334956</td>\n",
       "      <td>5.211968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6813.759545</td>\n",
       "      <td>8.408494e+10</td>\n",
       "      <td>1.570983e+24</td>\n",
       "      <td>1.053031e+14</td>\n",
       "      <td>1530.518892</td>\n",
       "      <td>4.714137e+13</td>\n",
       "      <td>6.541886e+22</td>\n",
       "      <td>-2.204741e+25</td>\n",
       "      <td>14125.375441</td>\n",
       "      <td>-0.197436</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414585</td>\n",
       "      <td>5.439978</td>\n",
       "      <td>5.349852</td>\n",
       "      <td>5.199410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6810.946741</td>\n",
       "      <td>8.387123e+10</td>\n",
       "      <td>1.559647e+24</td>\n",
       "      <td>1.054864e+14</td>\n",
       "      <td>1553.297953</td>\n",
       "      <td>5.192169e+13</td>\n",
       "      <td>6.534533e+22</td>\n",
       "      <td>-2.227891e+25</td>\n",
       "      <td>14030.816852</td>\n",
       "      <td>-0.200018</td>\n",
       "      <td>...</td>\n",
       "      <td>5.426539</td>\n",
       "      <td>5.439688</td>\n",
       "      <td>5.333861</td>\n",
       "      <td>5.220641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6750.173733</td>\n",
       "      <td>8.393793e+10</td>\n",
       "      <td>1.567801e+24</td>\n",
       "      <td>1.044780e+14</td>\n",
       "      <td>1529.499791</td>\n",
       "      <td>5.263473e+13</td>\n",
       "      <td>6.513396e+22</td>\n",
       "      <td>-2.198664e+25</td>\n",
       "      <td>14122.395674</td>\n",
       "      <td>-0.197237</td>\n",
       "      <td>...</td>\n",
       "      <td>5.427973</td>\n",
       "      <td>5.437507</td>\n",
       "      <td>5.321049</td>\n",
       "      <td>5.218753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6625.715466</td>\n",
       "      <td>8.361261e+10</td>\n",
       "      <td>1.570021e+24</td>\n",
       "      <td>1.039780e+14</td>\n",
       "      <td>1558.476742</td>\n",
       "      <td>5.476180e+13</td>\n",
       "      <td>6.486121e+22</td>\n",
       "      <td>-2.151174e+25</td>\n",
       "      <td>14176.476618</td>\n",
       "      <td>-0.193727</td>\n",
       "      <td>...</td>\n",
       "      <td>5.420675</td>\n",
       "      <td>5.402765</td>\n",
       "      <td>5.321893</td>\n",
       "      <td>5.207600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6602.456377</td>\n",
       "      <td>8.343825e+10</td>\n",
       "      <td>1.559968e+24</td>\n",
       "      <td>1.037375e+14</td>\n",
       "      <td>1475.698999</td>\n",
       "      <td>5.215698e+13</td>\n",
       "      <td>6.508146e+22</td>\n",
       "      <td>-2.175203e+25</td>\n",
       "      <td>13994.417647</td>\n",
       "      <td>-0.196301</td>\n",
       "      <td>...</td>\n",
       "      <td>5.424027</td>\n",
       "      <td>5.401567</td>\n",
       "      <td>5.328385</td>\n",
       "      <td>5.212209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6579.279348</td>\n",
       "      <td>8.332854e+10</td>\n",
       "      <td>1.556961e+24</td>\n",
       "      <td>1.027367e+14</td>\n",
       "      <td>1497.390470</td>\n",
       "      <td>5.335861e+13</td>\n",
       "      <td>6.486579e+22</td>\n",
       "      <td>-2.183151e+25</td>\n",
       "      <td>14085.404680</td>\n",
       "      <td>-0.197277</td>\n",
       "      <td>...</td>\n",
       "      <td>5.412794</td>\n",
       "      <td>5.392322</td>\n",
       "      <td>5.331879</td>\n",
       "      <td>5.229455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6659.340976</td>\n",
       "      <td>8.366482e+10</td>\n",
       "      <td>1.561306e+24</td>\n",
       "      <td>1.038385e+14</td>\n",
       "      <td>1595.699476</td>\n",
       "      <td>5.790126e+13</td>\n",
       "      <td>6.530352e+22</td>\n",
       "      <td>-2.204153e+25</td>\n",
       "      <td>13978.934717</td>\n",
       "      <td>-0.198375</td>\n",
       "      <td>...</td>\n",
       "      <td>5.407542</td>\n",
       "      <td>5.395212</td>\n",
       "      <td>5.338315</td>\n",
       "      <td>5.199087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2201.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6674.435798</td>\n",
       "      <td>8.363221e+10</td>\n",
       "      <td>1.558285e+24</td>\n",
       "      <td>1.049437e+14</td>\n",
       "      <td>1561.801278</td>\n",
       "      <td>5.628489e+13</td>\n",
       "      <td>6.543992e+22</td>\n",
       "      <td>-2.211603e+25</td>\n",
       "      <td>13883.545665</td>\n",
       "      <td>-0.199123</td>\n",
       "      <td>...</td>\n",
       "      <td>5.418874</td>\n",
       "      <td>5.404427</td>\n",
       "      <td>5.351370</td>\n",
       "      <td>5.195523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2147.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6628.940407</td>\n",
       "      <td>8.364418e+10</td>\n",
       "      <td>1.560219e+24</td>\n",
       "      <td>1.037925e+14</td>\n",
       "      <td>1558.677333</td>\n",
       "      <td>5.651785e+13</td>\n",
       "      <td>6.543197e+22</td>\n",
       "      <td>-2.211868e+25</td>\n",
       "      <td>13923.843184</td>\n",
       "      <td>-0.199118</td>\n",
       "      <td>...</td>\n",
       "      <td>5.429408</td>\n",
       "      <td>5.417444</td>\n",
       "      <td>5.352889</td>\n",
       "      <td>5.225211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2299.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6534.272066</td>\n",
       "      <td>8.347730e+10</td>\n",
       "      <td>1.564493e+24</td>\n",
       "      <td>1.040900e+14</td>\n",
       "      <td>1475.048207</td>\n",
       "      <td>5.209285e+13</td>\n",
       "      <td>6.540494e+22</td>\n",
       "      <td>-2.160368e+25</td>\n",
       "      <td>13833.285734</td>\n",
       "      <td>-0.194871</td>\n",
       "      <td>...</td>\n",
       "      <td>5.429178</td>\n",
       "      <td>5.405270</td>\n",
       "      <td>5.353649</td>\n",
       "      <td>5.195410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6618.672577</td>\n",
       "      <td>8.346451e+10</td>\n",
       "      <td>1.557323e+24</td>\n",
       "      <td>1.033482e+14</td>\n",
       "      <td>1600.107458</td>\n",
       "      <td>5.740748e+13</td>\n",
       "      <td>6.560208e+22</td>\n",
       "      <td>-2.194959e+25</td>\n",
       "      <td>13785.588862</td>\n",
       "      <td>-0.198021</td>\n",
       "      <td>...</td>\n",
       "      <td>5.446002</td>\n",
       "      <td>5.412405</td>\n",
       "      <td>5.345017</td>\n",
       "      <td>5.246542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6556.497525</td>\n",
       "      <td>8.331999e+10</td>\n",
       "      <td>1.555459e+24</td>\n",
       "      <td>1.036262e+14</td>\n",
       "      <td>1582.988346</td>\n",
       "      <td>5.571156e+13</td>\n",
       "      <td>6.565363e+22</td>\n",
       "      <td>-2.201092e+25</td>\n",
       "      <td>13704.966138</td>\n",
       "      <td>-0.198919</td>\n",
       "      <td>...</td>\n",
       "      <td>5.436352</td>\n",
       "      <td>5.416115</td>\n",
       "      <td>5.330692</td>\n",
       "      <td>5.228028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2316.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6664.787875</td>\n",
       "      <td>8.381583e+10</td>\n",
       "      <td>1.554123e+24</td>\n",
       "      <td>1.049621e+14</td>\n",
       "      <td>1607.631910</td>\n",
       "      <td>5.965254e+13</td>\n",
       "      <td>6.624869e+22</td>\n",
       "      <td>-2.268300e+25</td>\n",
       "      <td>13567.615361</td>\n",
       "      <td>-0.203780</td>\n",
       "      <td>...</td>\n",
       "      <td>5.449425</td>\n",
       "      <td>5.438875</td>\n",
       "      <td>5.323645</td>\n",
       "      <td>5.244872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6610.832931</td>\n",
       "      <td>8.372099e+10</td>\n",
       "      <td>1.551750e+24</td>\n",
       "      <td>1.045356e+14</td>\n",
       "      <td>1628.840026</td>\n",
       "      <td>5.812513e+13</td>\n",
       "      <td>6.629692e+22</td>\n",
       "      <td>-2.266189e+25</td>\n",
       "      <td>13518.841224</td>\n",
       "      <td>-0.203821</td>\n",
       "      <td>...</td>\n",
       "      <td>5.449262</td>\n",
       "      <td>5.424079</td>\n",
       "      <td>5.304066</td>\n",
       "      <td>5.238537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2203.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6623.903993</td>\n",
       "      <td>8.393815e+10</td>\n",
       "      <td>1.547466e+24</td>\n",
       "      <td>1.046448e+14</td>\n",
       "      <td>1681.735488</td>\n",
       "      <td>6.043938e+13</td>\n",
       "      <td>6.663268e+22</td>\n",
       "      <td>-2.302672e+25</td>\n",
       "      <td>13449.626827</td>\n",
       "      <td>-0.206567</td>\n",
       "      <td>...</td>\n",
       "      <td>5.438638</td>\n",
       "      <td>5.447932</td>\n",
       "      <td>5.311403</td>\n",
       "      <td>5.249751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6604.636688</td>\n",
       "      <td>8.384039e+10</td>\n",
       "      <td>1.543670e+24</td>\n",
       "      <td>1.042238e+14</td>\n",
       "      <td>1658.377172</td>\n",
       "      <td>5.433214e+13</td>\n",
       "      <td>6.649111e+22</td>\n",
       "      <td>-2.306179e+25</td>\n",
       "      <td>13492.628976</td>\n",
       "      <td>-0.207122</td>\n",
       "      <td>...</td>\n",
       "      <td>5.415847</td>\n",
       "      <td>5.465051</td>\n",
       "      <td>5.325372</td>\n",
       "      <td>5.225893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6590.454932</td>\n",
       "      <td>8.376975e+10</td>\n",
       "      <td>1.544275e+24</td>\n",
       "      <td>1.037572e+14</td>\n",
       "      <td>1656.194789</td>\n",
       "      <td>5.797011e+13</td>\n",
       "      <td>6.652554e+22</td>\n",
       "      <td>-2.285079e+25</td>\n",
       "      <td>13423.901150</td>\n",
       "      <td>-0.205400</td>\n",
       "      <td>...</td>\n",
       "      <td>5.436240</td>\n",
       "      <td>5.471888</td>\n",
       "      <td>5.326894</td>\n",
       "      <td>5.228644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2253.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6554.577321</td>\n",
       "      <td>8.382140e+10</td>\n",
       "      <td>1.551716e+24</td>\n",
       "      <td>1.037426e+14</td>\n",
       "      <td>1666.964946</td>\n",
       "      <td>5.595178e+13</td>\n",
       "      <td>6.659935e+22</td>\n",
       "      <td>-2.270023e+25</td>\n",
       "      <td>13490.144006</td>\n",
       "      <td>-0.203921</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421760</td>\n",
       "      <td>5.455594</td>\n",
       "      <td>5.327001</td>\n",
       "      <td>5.247861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2178.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6589.028665</td>\n",
       "      <td>8.362348e+10</td>\n",
       "      <td>1.548201e+24</td>\n",
       "      <td>1.047694e+14</td>\n",
       "      <td>1718.904126</td>\n",
       "      <td>5.865956e+13</td>\n",
       "      <td>6.661440e+22</td>\n",
       "      <td>-2.252770e+25</td>\n",
       "      <td>13382.792443</td>\n",
       "      <td>-0.202850</td>\n",
       "      <td>...</td>\n",
       "      <td>5.416107</td>\n",
       "      <td>5.438976</td>\n",
       "      <td>5.316460</td>\n",
       "      <td>5.241509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6696.230175</td>\n",
       "      <td>8.374229e+10</td>\n",
       "      <td>1.550484e+24</td>\n",
       "      <td>1.050534e+14</td>\n",
       "      <td>1780.950881</td>\n",
       "      <td>6.372367e+13</td>\n",
       "      <td>6.670024e+22</td>\n",
       "      <td>-2.260599e+25</td>\n",
       "      <td>13434.294831</td>\n",
       "      <td>-0.203267</td>\n",
       "      <td>...</td>\n",
       "      <td>5.403140</td>\n",
       "      <td>5.454633</td>\n",
       "      <td>5.323734</td>\n",
       "      <td>5.228264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6680.609878</td>\n",
       "      <td>8.378515e+10</td>\n",
       "      <td>1.540382e+24</td>\n",
       "      <td>1.042639e+14</td>\n",
       "      <td>1790.150343</td>\n",
       "      <td>6.220128e+13</td>\n",
       "      <td>6.695897e+22</td>\n",
       "      <td>-2.314251e+25</td>\n",
       "      <td>13321.778770</td>\n",
       "      <td>-0.207984</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405974</td>\n",
       "      <td>5.434141</td>\n",
       "      <td>5.301531</td>\n",
       "      <td>5.222481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6655.976645</td>\n",
       "      <td>8.391120e+10</td>\n",
       "      <td>1.542652e+24</td>\n",
       "      <td>1.037527e+14</td>\n",
       "      <td>1774.532620</td>\n",
       "      <td>6.236907e+13</td>\n",
       "      <td>6.709688e+22</td>\n",
       "      <td>-2.312158e+25</td>\n",
       "      <td>13351.836402</td>\n",
       "      <td>-0.207484</td>\n",
       "      <td>...</td>\n",
       "      <td>5.412006</td>\n",
       "      <td>5.447493</td>\n",
       "      <td>5.318548</td>\n",
       "      <td>5.231530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6621.583380</td>\n",
       "      <td>8.412847e+10</td>\n",
       "      <td>1.550473e+24</td>\n",
       "      <td>1.048586e+14</td>\n",
       "      <td>1828.370218</td>\n",
       "      <td>6.313825e+13</td>\n",
       "      <td>6.710731e+22</td>\n",
       "      <td>-2.288411e+25</td>\n",
       "      <td>13328.826569</td>\n",
       "      <td>-0.204823</td>\n",
       "      <td>...</td>\n",
       "      <td>5.399834</td>\n",
       "      <td>5.472706</td>\n",
       "      <td>5.313747</td>\n",
       "      <td>5.233165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2326.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6714.414943</td>\n",
       "      <td>8.417939e+10</td>\n",
       "      <td>1.533120e+24</td>\n",
       "      <td>1.041958e+14</td>\n",
       "      <td>1799.467489</td>\n",
       "      <td>6.292113e+13</td>\n",
       "      <td>6.729763e+22</td>\n",
       "      <td>-2.367600e+25</td>\n",
       "      <td>13291.640048</td>\n",
       "      <td>-0.211782</td>\n",
       "      <td>...</td>\n",
       "      <td>5.409866</td>\n",
       "      <td>5.463669</td>\n",
       "      <td>5.298373</td>\n",
       "      <td>5.224789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2385.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6713.854682</td>\n",
       "      <td>8.428436e+10</td>\n",
       "      <td>1.542590e+24</td>\n",
       "      <td>1.039199e+14</td>\n",
       "      <td>1784.416135</td>\n",
       "      <td>6.210627e+13</td>\n",
       "      <td>6.705084e+22</td>\n",
       "      <td>-2.349227e+25</td>\n",
       "      <td>13439.508935</td>\n",
       "      <td>-0.209877</td>\n",
       "      <td>...</td>\n",
       "      <td>5.431626</td>\n",
       "      <td>5.451033</td>\n",
       "      <td>5.297726</td>\n",
       "      <td>5.220776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2336.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6675.049946</td>\n",
       "      <td>8.433016e+10</td>\n",
       "      <td>1.538034e+24</td>\n",
       "      <td>1.040402e+14</td>\n",
       "      <td>1819.143184</td>\n",
       "      <td>6.488646e+13</td>\n",
       "      <td>6.720600e+22</td>\n",
       "      <td>-2.352803e+25</td>\n",
       "      <td>13388.201074</td>\n",
       "      <td>-0.210083</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421072</td>\n",
       "      <td>5.471903</td>\n",
       "      <td>5.294902</td>\n",
       "      <td>5.225364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2445.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6651.842005</td>\n",
       "      <td>8.457251e+10</td>\n",
       "      <td>1.546075e+24</td>\n",
       "      <td>1.035750e+14</td>\n",
       "      <td>1821.234321</td>\n",
       "      <td>6.246896e+13</td>\n",
       "      <td>6.683234e+22</td>\n",
       "      <td>-2.344465e+25</td>\n",
       "      <td>13559.769347</td>\n",
       "      <td>-0.208738</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413351</td>\n",
       "      <td>5.463487</td>\n",
       "      <td>5.295877</td>\n",
       "      <td>5.215056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6618.690252</td>\n",
       "      <td>8.479324e+10</td>\n",
       "      <td>1.556837e+24</td>\n",
       "      <td>1.026949e+14</td>\n",
       "      <td>1879.972741</td>\n",
       "      <td>6.718395e+13</td>\n",
       "      <td>6.662508e+22</td>\n",
       "      <td>-2.320355e+25</td>\n",
       "      <td>13744.924963</td>\n",
       "      <td>-0.206054</td>\n",
       "      <td>...</td>\n",
       "      <td>5.418516</td>\n",
       "      <td>5.460616</td>\n",
       "      <td>5.305683</td>\n",
       "      <td>5.225273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2574.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6689.582061</td>\n",
       "      <td>8.511911e+10</td>\n",
       "      <td>1.554712e+24</td>\n",
       "      <td>1.029952e+14</td>\n",
       "      <td>1920.438043</td>\n",
       "      <td>6.659065e+13</td>\n",
       "      <td>6.703677e+22</td>\n",
       "      <td>-2.359518e+25</td>\n",
       "      <td>13709.773165</td>\n",
       "      <td>-0.208729</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413922</td>\n",
       "      <td>5.463642</td>\n",
       "      <td>5.314288</td>\n",
       "      <td>5.241342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6651.957058</td>\n",
       "      <td>8.528108e+10</td>\n",
       "      <td>1.556176e+24</td>\n",
       "      <td>1.023277e+14</td>\n",
       "      <td>1799.937489</td>\n",
       "      <td>6.106326e+13</td>\n",
       "      <td>6.719677e+22</td>\n",
       "      <td>-2.369129e+25</td>\n",
       "      <td>13673.206273</td>\n",
       "      <td>-0.209182</td>\n",
       "      <td>...</td>\n",
       "      <td>5.415397</td>\n",
       "      <td>5.484200</td>\n",
       "      <td>5.318812</td>\n",
       "      <td>5.233028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2660.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6729.800739</td>\n",
       "      <td>8.549333e+10</td>\n",
       "      <td>1.566051e+24</td>\n",
       "      <td>1.030126e+14</td>\n",
       "      <td>1810.811383</td>\n",
       "      <td>6.165691e+13</td>\n",
       "      <td>6.719766e+22</td>\n",
       "      <td>-2.351989e+25</td>\n",
       "      <td>13800.878660</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>...</td>\n",
       "      <td>5.423642</td>\n",
       "      <td>5.449615</td>\n",
       "      <td>5.301622</td>\n",
       "      <td>5.208788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2513.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6749.666713</td>\n",
       "      <td>8.547887e+10</td>\n",
       "      <td>1.565630e+24</td>\n",
       "      <td>1.027505e+14</td>\n",
       "      <td>1816.423463</td>\n",
       "      <td>6.445213e+13</td>\n",
       "      <td>6.702058e+22</td>\n",
       "      <td>-2.357341e+25</td>\n",
       "      <td>13929.375838</td>\n",
       "      <td>-0.207659</td>\n",
       "      <td>...</td>\n",
       "      <td>5.419790</td>\n",
       "      <td>5.491169</td>\n",
       "      <td>5.298181</td>\n",
       "      <td>5.240031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2672.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6712.981030</td>\n",
       "      <td>8.579096e+10</td>\n",
       "      <td>1.570802e+24</td>\n",
       "      <td>1.032009e+14</td>\n",
       "      <td>1875.490188</td>\n",
       "      <td>6.395814e+13</td>\n",
       "      <td>6.737582e+22</td>\n",
       "      <td>-2.368335e+25</td>\n",
       "      <td>13878.799260</td>\n",
       "      <td>-0.207869</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414254</td>\n",
       "      <td>5.486133</td>\n",
       "      <td>5.296874</td>\n",
       "      <td>5.243491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2725.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6694.946004</td>\n",
       "      <td>8.595762e+10</td>\n",
       "      <td>1.565608e+24</td>\n",
       "      <td>1.032871e+14</td>\n",
       "      <td>1714.993210</td>\n",
       "      <td>5.665472e+13</td>\n",
       "      <td>6.773257e+22</td>\n",
       "      <td>-2.408134e+25</td>\n",
       "      <td>13738.599523</td>\n",
       "      <td>-0.210952</td>\n",
       "      <td>...</td>\n",
       "      <td>5.408871</td>\n",
       "      <td>5.489321</td>\n",
       "      <td>5.307261</td>\n",
       "      <td>5.219881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2610.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6720.925432</td>\n",
       "      <td>8.642296e+10</td>\n",
       "      <td>1.568783e+24</td>\n",
       "      <td>1.033549e+14</td>\n",
       "      <td>1788.925676</td>\n",
       "      <td>6.013856e+13</td>\n",
       "      <td>6.814229e+22</td>\n",
       "      <td>-2.453392e+25</td>\n",
       "      <td>13722.647950</td>\n",
       "      <td>-0.213759</td>\n",
       "      <td>...</td>\n",
       "      <td>5.410845</td>\n",
       "      <td>5.464945</td>\n",
       "      <td>5.290419</td>\n",
       "      <td>5.235641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2616.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6643.833325</td>\n",
       "      <td>8.686979e+10</td>\n",
       "      <td>1.585399e+24</td>\n",
       "      <td>1.027543e+14</td>\n",
       "      <td>1811.564073</td>\n",
       "      <td>5.910807e+13</td>\n",
       "      <td>6.827774e+22</td>\n",
       "      <td>-2.426738e+25</td>\n",
       "      <td>13834.722831</td>\n",
       "      <td>-0.210350</td>\n",
       "      <td>...</td>\n",
       "      <td>5.415777</td>\n",
       "      <td>5.462901</td>\n",
       "      <td>5.289332</td>\n",
       "      <td>5.233490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2473.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6678.435053</td>\n",
       "      <td>8.726270e+10</td>\n",
       "      <td>1.588082e+24</td>\n",
       "      <td>1.029321e+14</td>\n",
       "      <td>1888.605961</td>\n",
       "      <td>6.151702e+13</td>\n",
       "      <td>6.895435e+22</td>\n",
       "      <td>-2.466136e+25</td>\n",
       "      <td>13750.892245</td>\n",
       "      <td>-0.212802</td>\n",
       "      <td>...</td>\n",
       "      <td>5.407051</td>\n",
       "      <td>5.498585</td>\n",
       "      <td>5.300528</td>\n",
       "      <td>5.232761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2302.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6719.481170</td>\n",
       "      <td>8.771953e+10</td>\n",
       "      <td>1.595610e+24</td>\n",
       "      <td>1.032735e+14</td>\n",
       "      <td>1839.960188</td>\n",
       "      <td>6.065092e+13</td>\n",
       "      <td>6.955306e+22</td>\n",
       "      <td>-2.491348e+25</td>\n",
       "      <td>13604.712481</td>\n",
       "      <td>-0.213858</td>\n",
       "      <td>...</td>\n",
       "      <td>5.430693</td>\n",
       "      <td>5.476389</td>\n",
       "      <td>5.304190</td>\n",
       "      <td>5.233783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2460.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>6731.164211</td>\n",
       "      <td>8.808078e+10</td>\n",
       "      <td>1.601331e+24</td>\n",
       "      <td>1.040559e+14</td>\n",
       "      <td>1870.737825</td>\n",
       "      <td>6.834156e+13</td>\n",
       "      <td>6.991464e+22</td>\n",
       "      <td>-2.486756e+25</td>\n",
       "      <td>13517.112435</td>\n",
       "      <td>-0.212588</td>\n",
       "      <td>...</td>\n",
       "      <td>5.434391</td>\n",
       "      <td>5.482465</td>\n",
       "      <td>5.301332</td>\n",
       "      <td>5.229522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2138.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6743.632587</td>\n",
       "      <td>8.818426e+10</td>\n",
       "      <td>1.600107e+24</td>\n",
       "      <td>1.048489e+14</td>\n",
       "      <td>1870.199589</td>\n",
       "      <td>6.936537e+13</td>\n",
       "      <td>7.029250e+22</td>\n",
       "      <td>-2.508392e+25</td>\n",
       "      <td>13356.904229</td>\n",
       "      <td>-0.214186</td>\n",
       "      <td>...</td>\n",
       "      <td>5.425442</td>\n",
       "      <td>5.483249</td>\n",
       "      <td>5.316916</td>\n",
       "      <td>5.207895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>6873.947508</td>\n",
       "      <td>8.867265e+10</td>\n",
       "      <td>1.605313e+24</td>\n",
       "      <td>1.060069e+14</td>\n",
       "      <td>1904.759793</td>\n",
       "      <td>6.729846e+13</td>\n",
       "      <td>7.111755e+22</td>\n",
       "      <td>-2.558268e+25</td>\n",
       "      <td>13233.412164</td>\n",
       "      <td>-0.217242</td>\n",
       "      <td>...</td>\n",
       "      <td>5.428043</td>\n",
       "      <td>5.480052</td>\n",
       "      <td>5.321115</td>\n",
       "      <td>5.230335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6901.910797</td>\n",
       "      <td>8.892919e+10</td>\n",
       "      <td>1.600205e+24</td>\n",
       "      <td>1.069829e+14</td>\n",
       "      <td>1909.576325</td>\n",
       "      <td>6.973227e+13</td>\n",
       "      <td>7.179346e+22</td>\n",
       "      <td>-2.599559e+25</td>\n",
       "      <td>13050.715628</td>\n",
       "      <td>-0.220111</td>\n",
       "      <td>...</td>\n",
       "      <td>5.434801</td>\n",
       "      <td>5.483810</td>\n",
       "      <td>5.317496</td>\n",
       "      <td>5.226047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6951.264001</td>\n",
       "      <td>8.907855e+10</td>\n",
       "      <td>1.597570e+24</td>\n",
       "      <td>1.084181e+14</td>\n",
       "      <td>1931.615401</td>\n",
       "      <td>7.088334e+13</td>\n",
       "      <td>7.229525e+22</td>\n",
       "      <td>-2.639231e+25</td>\n",
       "      <td>12883.123491</td>\n",
       "      <td>-0.223096</td>\n",
       "      <td>...</td>\n",
       "      <td>5.433837</td>\n",
       "      <td>5.479156</td>\n",
       "      <td>5.305367</td>\n",
       "      <td>5.208082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6916.164823</td>\n",
       "      <td>8.879042e+10</td>\n",
       "      <td>1.595188e+24</td>\n",
       "      <td>1.082073e+14</td>\n",
       "      <td>1893.966861</td>\n",
       "      <td>6.963535e+13</td>\n",
       "      <td>7.229235e+22</td>\n",
       "      <td>-2.627192e+25</td>\n",
       "      <td>12791.450513</td>\n",
       "      <td>-0.222799</td>\n",
       "      <td>...</td>\n",
       "      <td>5.435923</td>\n",
       "      <td>5.495387</td>\n",
       "      <td>5.309339</td>\n",
       "      <td>5.212751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6954.711005</td>\n",
       "      <td>8.892432e+10</td>\n",
       "      <td>1.585478e+24</td>\n",
       "      <td>1.077902e+14</td>\n",
       "      <td>1915.377016</td>\n",
       "      <td>7.154282e+13</td>\n",
       "      <td>7.282973e+22</td>\n",
       "      <td>-2.687569e+25</td>\n",
       "      <td>12666.103787</td>\n",
       "      <td>-0.227576</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405683</td>\n",
       "      <td>5.495611</td>\n",
       "      <td>5.322693</td>\n",
       "      <td>5.194471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3            4   \\\n",
       "0   6748.177901  8.294421e+10  1.591225e+24  1.095967e+14  1510.190735   \n",
       "1   6713.245495  8.301113e+10  1.591180e+24  1.082126e+14  1552.899870   \n",
       "2   6711.746592  8.324237e+10  1.585508e+24  1.074454e+14  1569.593570   \n",
       "3   6903.974288  8.343257e+10  1.576556e+24  1.084795e+14  1608.218901   \n",
       "4   6691.365852  8.370511e+10  1.600757e+24  1.086643e+14  1479.272564   \n",
       "5   6834.958217  8.406158e+10  1.591036e+24  1.096684e+14  1644.488148   \n",
       "6   6830.348029  8.431275e+10  1.589851e+24  1.083214e+14  1587.280389   \n",
       "7   6894.216660  8.429531e+10  1.577024e+24  1.090471e+14  1530.971594   \n",
       "8   6906.079329  8.449927e+10  1.586829e+24  1.088327e+14  1580.642155   \n",
       "9   6899.094797  8.451908e+10  1.601676e+24  1.091217e+14  1562.139216   \n",
       "10  6911.805621  8.453654e+10  1.584634e+24  1.085458e+14  1608.232234   \n",
       "11  6946.807156  8.448848e+10  1.588865e+24  1.085662e+14  1598.537641   \n",
       "12  6886.818627  8.431700e+10  1.578638e+24  1.071168e+14  1581.500858   \n",
       "13  6818.410837  8.425498e+10  1.577545e+24  1.059483e+14  1552.157851   \n",
       "14  6837.526272  8.424089e+10  1.577035e+24  1.051378e+14  1623.889248   \n",
       "15  6813.759545  8.408494e+10  1.570983e+24  1.053031e+14  1530.518892   \n",
       "16  6810.946741  8.387123e+10  1.559647e+24  1.054864e+14  1553.297953   \n",
       "17  6750.173733  8.393793e+10  1.567801e+24  1.044780e+14  1529.499791   \n",
       "18  6625.715466  8.361261e+10  1.570021e+24  1.039780e+14  1558.476742   \n",
       "19  6602.456377  8.343825e+10  1.559968e+24  1.037375e+14  1475.698999   \n",
       "20  6579.279348  8.332854e+10  1.556961e+24  1.027367e+14  1497.390470   \n",
       "21  6659.340976  8.366482e+10  1.561306e+24  1.038385e+14  1595.699476   \n",
       "22  6674.435798  8.363221e+10  1.558285e+24  1.049437e+14  1561.801278   \n",
       "23  6628.940407  8.364418e+10  1.560219e+24  1.037925e+14  1558.677333   \n",
       "24  6534.272066  8.347730e+10  1.564493e+24  1.040900e+14  1475.048207   \n",
       "25  6618.672577  8.346451e+10  1.557323e+24  1.033482e+14  1600.107458   \n",
       "26  6556.497525  8.331999e+10  1.555459e+24  1.036262e+14  1582.988346   \n",
       "27  6664.787875  8.381583e+10  1.554123e+24  1.049621e+14  1607.631910   \n",
       "28  6610.832931  8.372099e+10  1.551750e+24  1.045356e+14  1628.840026   \n",
       "29  6623.903993  8.393815e+10  1.547466e+24  1.046448e+14  1681.735488   \n",
       "30  6604.636688  8.384039e+10  1.543670e+24  1.042238e+14  1658.377172   \n",
       "31  6590.454932  8.376975e+10  1.544275e+24  1.037572e+14  1656.194789   \n",
       "32  6554.577321  8.382140e+10  1.551716e+24  1.037426e+14  1666.964946   \n",
       "33  6589.028665  8.362348e+10  1.548201e+24  1.047694e+14  1718.904126   \n",
       "34  6696.230175  8.374229e+10  1.550484e+24  1.050534e+14  1780.950881   \n",
       "35  6680.609878  8.378515e+10  1.540382e+24  1.042639e+14  1790.150343   \n",
       "36  6655.976645  8.391120e+10  1.542652e+24  1.037527e+14  1774.532620   \n",
       "37  6621.583380  8.412847e+10  1.550473e+24  1.048586e+14  1828.370218   \n",
       "38  6714.414943  8.417939e+10  1.533120e+24  1.041958e+14  1799.467489   \n",
       "39  6713.854682  8.428436e+10  1.542590e+24  1.039199e+14  1784.416135   \n",
       "40  6675.049946  8.433016e+10  1.538034e+24  1.040402e+14  1819.143184   \n",
       "41  6651.842005  8.457251e+10  1.546075e+24  1.035750e+14  1821.234321   \n",
       "42  6618.690252  8.479324e+10  1.556837e+24  1.026949e+14  1879.972741   \n",
       "43  6689.582061  8.511911e+10  1.554712e+24  1.029952e+14  1920.438043   \n",
       "44  6651.957058  8.528108e+10  1.556176e+24  1.023277e+14  1799.937489   \n",
       "45  6729.800739  8.549333e+10  1.566051e+24  1.030126e+14  1810.811383   \n",
       "46  6749.666713  8.547887e+10  1.565630e+24  1.027505e+14  1816.423463   \n",
       "47  6712.981030  8.579096e+10  1.570802e+24  1.032009e+14  1875.490188   \n",
       "48  6694.946004  8.595762e+10  1.565608e+24  1.032871e+14  1714.993210   \n",
       "49  6720.925432  8.642296e+10  1.568783e+24  1.033549e+14  1788.925676   \n",
       "50  6643.833325  8.686979e+10  1.585399e+24  1.027543e+14  1811.564073   \n",
       "51  6678.435053  8.726270e+10  1.588082e+24  1.029321e+14  1888.605961   \n",
       "52  6719.481170  8.771953e+10  1.595610e+24  1.032735e+14  1839.960188   \n",
       "53  6731.164211  8.808078e+10  1.601331e+24  1.040559e+14  1870.737825   \n",
       "54  6743.632587  8.818426e+10  1.600107e+24  1.048489e+14  1870.199589   \n",
       "55  6873.947508  8.867265e+10  1.605313e+24  1.060069e+14  1904.759793   \n",
       "56  6901.910797  8.892919e+10  1.600205e+24  1.069829e+14  1909.576325   \n",
       "57  6951.264001  8.907855e+10  1.597570e+24  1.084181e+14  1931.615401   \n",
       "58  6916.164823  8.879042e+10  1.595188e+24  1.082073e+14  1893.966861   \n",
       "59  6954.711005  8.892432e+10  1.585478e+24  1.077902e+14  1915.377016   \n",
       "\n",
       "              5             6             7             8         9   ...  \\\n",
       "0   5.537032e+13  6.524454e+22 -1.945966e+25  13461.392092 -0.176659  ...   \n",
       "1   5.163912e+13  6.552311e+22 -1.967367e+25  13459.043822 -0.178458  ...   \n",
       "2   5.208053e+13  6.582194e+22 -2.022283e+25  13391.359484 -0.182930  ...   \n",
       "3   5.166028e+13  6.609184e+22 -2.072753e+25  13248.436229 -0.187068  ...   \n",
       "4   5.030951e+13  6.614713e+22 -1.999304e+25  13367.513454 -0.179852  ...   \n",
       "5   5.663543e+13  6.630166e+22 -2.075205e+25  13298.131406 -0.185888  ...   \n",
       "6   5.648788e+13  6.649235e+22 -2.105215e+25  13390.341243 -0.188014  ...   \n",
       "7   5.153441e+13  6.685179e+22 -2.175302e+25  13251.479423 -0.194314  ...   \n",
       "8   5.204076e+13  6.646028e+22 -2.155312e+25  13485.254521 -0.192063  ...   \n",
       "9   5.105149e+13  6.593766e+22 -2.084657e+25  13729.350076 -0.185724  ...   \n",
       "10  5.032479e+13  6.626648e+22 -2.168199e+25  13676.366262 -0.193127  ...   \n",
       "11  4.720406e+13  6.604935e+22 -2.160836e+25  13798.923015 -0.192580  ...   \n",
       "12  4.759987e+13  6.570224e+22 -2.183161e+25  13867.168557 -0.194966  ...   \n",
       "13  4.509200e+13  6.554720e+22 -2.179559e+25  14013.525138 -0.194787  ...   \n",
       "14  4.995272e+13  6.540712e+22 -2.187353e+25  14092.449955 -0.195516  ...   \n",
       "15  4.714137e+13  6.541886e+22 -2.204741e+25  14125.375441 -0.197436  ...   \n",
       "16  5.192169e+13  6.534533e+22 -2.227891e+25  14030.816852 -0.200018  ...   \n",
       "17  5.263473e+13  6.513396e+22 -2.198664e+25  14122.395674 -0.197237  ...   \n",
       "18  5.476180e+13  6.486121e+22 -2.151174e+25  14176.476618 -0.193727  ...   \n",
       "19  5.215698e+13  6.508146e+22 -2.175203e+25  13994.417647 -0.196301  ...   \n",
       "20  5.335861e+13  6.486579e+22 -2.183151e+25  14085.404680 -0.197277  ...   \n",
       "21  5.790126e+13  6.530352e+22 -2.204153e+25  13978.934717 -0.198375  ...   \n",
       "22  5.628489e+13  6.543992e+22 -2.211603e+25  13883.545665 -0.199123  ...   \n",
       "23  5.651785e+13  6.543197e+22 -2.211868e+25  13923.843184 -0.199118  ...   \n",
       "24  5.209285e+13  6.540494e+22 -2.160368e+25  13833.285734 -0.194871  ...   \n",
       "25  5.740748e+13  6.560208e+22 -2.194959e+25  13785.588862 -0.198021  ...   \n",
       "26  5.571156e+13  6.565363e+22 -2.201092e+25  13704.966138 -0.198919  ...   \n",
       "27  5.965254e+13  6.624869e+22 -2.268300e+25  13567.615361 -0.203780  ...   \n",
       "28  5.812513e+13  6.629692e+22 -2.266189e+25  13518.841224 -0.203821  ...   \n",
       "29  6.043938e+13  6.663268e+22 -2.302672e+25  13449.626827 -0.206567  ...   \n",
       "30  5.433214e+13  6.649111e+22 -2.306179e+25  13492.628976 -0.207122  ...   \n",
       "31  5.797011e+13  6.652554e+22 -2.285079e+25  13423.901150 -0.205400  ...   \n",
       "32  5.595178e+13  6.659935e+22 -2.270023e+25  13490.144006 -0.203921  ...   \n",
       "33  5.865956e+13  6.661440e+22 -2.252770e+25  13382.792443 -0.202850  ...   \n",
       "34  6.372367e+13  6.670024e+22 -2.260599e+25  13434.294831 -0.203267  ...   \n",
       "35  6.220128e+13  6.695897e+22 -2.314251e+25  13321.778770 -0.207984  ...   \n",
       "36  6.236907e+13  6.709688e+22 -2.312158e+25  13351.836402 -0.207484  ...   \n",
       "37  6.313825e+13  6.710731e+22 -2.288411e+25  13328.826569 -0.204823  ...   \n",
       "38  6.292113e+13  6.729763e+22 -2.367600e+25  13291.640048 -0.211782  ...   \n",
       "39  6.210627e+13  6.705084e+22 -2.349227e+25  13439.508935 -0.209877  ...   \n",
       "40  6.488646e+13  6.720600e+22 -2.352803e+25  13388.201074 -0.210083  ...   \n",
       "41  6.246896e+13  6.683234e+22 -2.344465e+25  13559.769347 -0.208738  ...   \n",
       "42  6.718395e+13  6.662508e+22 -2.320355e+25  13744.924963 -0.206054  ...   \n",
       "43  6.659065e+13  6.703677e+22 -2.359518e+25  13709.773165 -0.208729  ...   \n",
       "44  6.106326e+13  6.719677e+22 -2.369129e+25  13673.206273 -0.209182  ...   \n",
       "45  6.165691e+13  6.719766e+22 -2.351989e+25  13800.878660 -0.207153  ...   \n",
       "46  6.445213e+13  6.702058e+22 -2.357341e+25  13929.375838 -0.207659  ...   \n",
       "47  6.395814e+13  6.737582e+22 -2.368335e+25  13878.799260 -0.207869  ...   \n",
       "48  5.665472e+13  6.773257e+22 -2.408134e+25  13738.599523 -0.210952  ...   \n",
       "49  6.013856e+13  6.814229e+22 -2.453392e+25  13722.647950 -0.213759  ...   \n",
       "50  5.910807e+13  6.827774e+22 -2.426738e+25  13834.722831 -0.210350  ...   \n",
       "51  6.151702e+13  6.895435e+22 -2.466136e+25  13750.892245 -0.212802  ...   \n",
       "52  6.065092e+13  6.955306e+22 -2.491348e+25  13604.712481 -0.213858  ...   \n",
       "53  6.834156e+13  6.991464e+22 -2.486756e+25  13517.112435 -0.212588  ...   \n",
       "54  6.936537e+13  7.029250e+22 -2.508392e+25  13356.904229 -0.214186  ...   \n",
       "55  6.729846e+13  7.111755e+22 -2.558268e+25  13233.412164 -0.217242  ...   \n",
       "56  6.973227e+13  7.179346e+22 -2.599559e+25  13050.715628 -0.220111  ...   \n",
       "57  7.088334e+13  7.229525e+22 -2.639231e+25  12883.123491 -0.223096  ...   \n",
       "58  6.963535e+13  7.229235e+22 -2.627192e+25  12791.450513 -0.222799  ...   \n",
       "59  7.154282e+13  7.282973e+22 -2.687569e+25  12666.103787 -0.227576  ...   \n",
       "\n",
       "          23        24        25        26   27   28        29        30  \\\n",
       "0   5.414326  5.392323  5.336158  5.205328  0.0  0.0  0.000000  0.000000   \n",
       "1   5.405846  5.403088  5.335880  5.211822  0.0  0.0  0.000000  0.000000   \n",
       "2   5.405772  5.398195  5.345981  5.216945  0.0  0.0  0.000000  0.000039   \n",
       "3   5.396873  5.398732  5.327560  5.203755  0.0  0.0  0.000000  0.000000   \n",
       "4   5.426061  5.389834  5.351214  5.217820  0.0  0.0  0.000000  0.000000   \n",
       "5   5.421926  5.407266  5.350388  5.219855  0.0  0.0  0.000000  0.000000   \n",
       "6   5.419279  5.413495  5.353073  5.232006  0.0  0.0  0.000039  0.000000   \n",
       "7   5.413199  5.418618  5.341884  5.221831  0.0  0.0  0.000000  0.000000   \n",
       "8   5.419249  5.426273  5.354147  5.243087  0.0  0.0  0.000000  0.000000   \n",
       "9   5.427837  5.393532  5.321879  5.221579  0.0  0.0  0.000000  0.000000   \n",
       "10  5.421960  5.419106  5.349870  5.231510  0.0  0.0  0.000000  0.000000   \n",
       "11  5.409650  5.423591  5.336414  5.216673  0.0  0.0  0.000000  0.000000   \n",
       "12  5.404884  5.435943  5.344661  5.215722  0.0  0.0  0.000000  0.000000   \n",
       "13  5.395443  5.426074  5.338376  5.218632  0.0  0.0  0.000000  0.000000   \n",
       "14  5.414608  5.430777  5.334956  5.211968  0.0  0.0  0.000000  0.000000   \n",
       "15  5.414585  5.439978  5.349852  5.199410  0.0  0.0  0.000000  0.000000   \n",
       "16  5.426539  5.439688  5.333861  5.220641  0.0  0.0  0.000000  0.000000   \n",
       "17  5.427973  5.437507  5.321049  5.218753  0.0  0.0  0.000000  0.000000   \n",
       "18  5.420675  5.402765  5.321893  5.207600  0.0  0.0  0.000000  0.000000   \n",
       "19  5.424027  5.401567  5.328385  5.212209  0.0  0.0  0.000000  0.000000   \n",
       "20  5.412794  5.392322  5.331879  5.229455  0.0  0.0  0.000000  0.000000   \n",
       "21  5.407542  5.395212  5.338315  5.199087  0.0  0.0  0.000000  0.000000   \n",
       "22  5.418874  5.404427  5.351370  5.195523  0.0  0.0  0.000000  0.000000   \n",
       "23  5.429408  5.417444  5.352889  5.225211  0.0  0.0  0.000000  0.000000   \n",
       "24  5.429178  5.405270  5.353649  5.195410  0.0  0.0  0.000000  0.000000   \n",
       "25  5.446002  5.412405  5.345017  5.246542  0.0  0.0  0.000000  0.000000   \n",
       "26  5.436352  5.416115  5.330692  5.228028  0.0  0.0  0.000000  0.000000   \n",
       "27  5.449425  5.438875  5.323645  5.244872  0.0  0.0  0.000000  0.000000   \n",
       "28  5.449262  5.424079  5.304066  5.238537  0.0  0.0  0.000000  0.000000   \n",
       "29  5.438638  5.447932  5.311403  5.249751  0.0  0.0  0.000000  0.000000   \n",
       "30  5.415847  5.465051  5.325372  5.225893  0.0  0.0  0.000000  0.000000   \n",
       "31  5.436240  5.471888  5.326894  5.228644  0.0  0.0  0.000000  0.000000   \n",
       "32  5.421760  5.455594  5.327001  5.247861  0.0  0.0  0.000000  0.000000   \n",
       "33  5.416107  5.438976  5.316460  5.241509  0.0  0.0  0.000000  0.000000   \n",
       "34  5.403140  5.454633  5.323734  5.228264  0.0  0.0  0.000000  0.000000   \n",
       "35  5.405974  5.434141  5.301531  5.222481  0.0  0.0  0.000000  0.000000   \n",
       "36  5.412006  5.447493  5.318548  5.231530  0.0  0.0  0.000000  0.000000   \n",
       "37  5.399834  5.472706  5.313747  5.233165  0.0  0.0  0.000000  0.000000   \n",
       "38  5.409866  5.463669  5.298373  5.224789  0.0  0.0  0.000000  0.000000   \n",
       "39  5.431626  5.451033  5.297726  5.220776  0.0  0.0  0.000000  0.000000   \n",
       "40  5.421072  5.471903  5.294902  5.225364  0.0  0.0  0.000000  0.000000   \n",
       "41  5.413351  5.463487  5.295877  5.215056  0.0  0.0  0.000000  0.000000   \n",
       "42  5.418516  5.460616  5.305683  5.225273  0.0  0.0  0.000000  0.000000   \n",
       "43  5.413922  5.463642  5.314288  5.241342  0.0  0.0  0.000000  0.000000   \n",
       "44  5.415397  5.484200  5.318812  5.233028  0.0  0.0  0.000000  0.000000   \n",
       "45  5.423642  5.449615  5.301622  5.208788  0.0  0.0  0.000000  0.000000   \n",
       "46  5.419790  5.491169  5.298181  5.240031  0.0  0.0  0.000000  0.000000   \n",
       "47  5.414254  5.486133  5.296874  5.243491  0.0  0.0  0.000000  0.000000   \n",
       "48  5.408871  5.489321  5.307261  5.219881  0.0  0.0  0.000000  0.000000   \n",
       "49  5.410845  5.464945  5.290419  5.235641  0.0  0.0  0.000000  0.000000   \n",
       "50  5.415777  5.462901  5.289332  5.233490  0.0  0.0  0.000000  0.000000   \n",
       "51  5.407051  5.498585  5.300528  5.232761  0.0  0.0  0.000000  0.000000   \n",
       "52  5.430693  5.476389  5.304190  5.233783  0.0  0.0  0.000000  0.000000   \n",
       "53  5.434391  5.482465  5.301332  5.229522  0.0  0.0  0.000000  0.000000   \n",
       "54  5.425442  5.483249  5.316916  5.207895  0.0  0.0  0.000000  0.000000   \n",
       "55  5.428043  5.480052  5.321115  5.230335  0.0  0.0  0.000000  0.000039   \n",
       "56  5.434801  5.483810  5.317496  5.226047  0.0  0.0  0.000000  0.000000   \n",
       "57  5.433837  5.479156  5.305367  5.208082  0.0  0.0  0.000000  0.000000   \n",
       "58  5.435923  5.495387  5.309339  5.212751  0.0  0.0  0.000000  0.000000   \n",
       "59  5.405683  5.495611  5.322693  5.194471  0.0  0.0  0.000000  0.000000   \n",
       "\n",
       "        31        32  \n",
       "0   1424.0  0.000004  \n",
       "1   1400.0  0.000007  \n",
       "2   1405.0  0.000003  \n",
       "3   1429.0  0.000003  \n",
       "4   1430.0  0.000003  \n",
       "5   1517.0  0.000004  \n",
       "6   1477.0  0.000004  \n",
       "7   1599.0  0.000015  \n",
       "8   1585.0  0.000008  \n",
       "9   1726.0  0.000005  \n",
       "10  1865.0  0.000004  \n",
       "11  1854.0  0.000004  \n",
       "12  1874.0  0.000004  \n",
       "13  1839.0  0.000003  \n",
       "14  1876.0  0.000002  \n",
       "15  1840.0  0.000002  \n",
       "16  1923.0  0.000005  \n",
       "17  1930.0  0.000019  \n",
       "18  1917.0  0.000016  \n",
       "19  1972.0  0.000004  \n",
       "20  2032.0  0.000003  \n",
       "21  2201.0  0.000003  \n",
       "22  2147.0  0.000003  \n",
       "23  2299.0  0.000003  \n",
       "24  2214.0  0.000003  \n",
       "25  2193.0  0.000003  \n",
       "26  2316.0  0.000003  \n",
       "27  2226.0  0.000004  \n",
       "28  2203.0  0.000004  \n",
       "29  2164.0  0.000003  \n",
       "30  2276.0  0.000003  \n",
       "31  2253.0  0.000003  \n",
       "32  2178.0  0.000005  \n",
       "33  2132.0  0.000013  \n",
       "34  2123.0  0.000004  \n",
       "35  2300.0  0.000003  \n",
       "36  2250.0  0.000003  \n",
       "37  2326.0  0.000003  \n",
       "38  2385.0  0.000003  \n",
       "39  2336.0  0.000004  \n",
       "40  2445.0  0.000008  \n",
       "41  2376.0  0.000007  \n",
       "42  2574.0  0.000004  \n",
       "43  2595.0  0.000004  \n",
       "44  2660.0  0.000003  \n",
       "45  2513.0  0.000005  \n",
       "46  2672.0  0.000005  \n",
       "47  2725.0  0.000003  \n",
       "48  2610.0  0.000003  \n",
       "49  2616.0  0.000003  \n",
       "50  2473.0  0.000003  \n",
       "51  2302.0  0.000003  \n",
       "52  2460.0  0.000003  \n",
       "53  2138.0  0.000005  \n",
       "54  2230.0  0.000007  \n",
       "55  2280.0  0.000003  \n",
       "56  2132.0  0.000002  \n",
       "57  2230.0  0.000002  \n",
       "58  2297.0  0.000003  \n",
       "59  2188.0  0.000007  \n",
       "\n",
       "[60 rows x 33 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temptrainData=np.empty([1540,60, 33])\n",
    "n=len(trainData)\n",
    "for l in range(0, n):\n",
    "  temp=trainData[l]\n",
    "  #print(temp)\n",
    "  #temp=np.transpose(temp)\n",
    "  temp=temp.T\n",
    "  #print(temp.shape)\n",
    "  #print(temp)\n",
    "  temptrainData[l,:,:]=temp\n",
    "  n=n+1\n",
    "  #np.append(temptrainData, temp)\n",
    "  #print(temptrainData)\n",
    "\n",
    "#print(temptrainData.shape)\n",
    "#print(trainData.shape) \n",
    "trainData=temptrainData\n",
    "print(\"trainData.shape: \",trainData.shape)\n",
    "#print(trainData[0])\n",
    "\n",
    "temp=trainData[0]\n",
    "#print(temp)\n",
    "df = pd.DataFrame(temp)\n",
    "#df=pd.DataFrame.from_dict(trainData)\n",
    "trainData222=trainData\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "529b6115-0cb7-4dcb-bc92-0ded7c1797b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1540, 60, 33)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "print(trainData.shape)\n",
    "\n",
    "print(type(trainData))\n",
    "npArrays=[]\n",
    "for l in range(0, len(trainData)):\n",
    "  trainData_std = sc.fit_transform(trainData[l])\n",
    "  npArrays.append(trainData_std)\n",
    "\n",
    "print(type(npArrays))\n",
    "arr = np.asarray(npArrays)\n",
    "print(type(arr))\n",
    "trainData_scaled=arr\n",
    "df = pd.DataFrame(trainData_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60478e14-2a03-4aec-8498-7641214e4c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1540, 264)\n"
     ]
    }
   ],
   "source": [
    "num_mvts=trainData_scaled.shape[0]\n",
    "num_ts=trainData_scaled.shape[1]\n",
    "num_params=trainData_scaled.shape[2]\n",
    "\n",
    "#num_mvts=2\n",
    "#num_mvts=len(trainData_scaled)\n",
    "x = np.zeros( (num_mvts,num_params*8) )\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dae2bc65-648d-47bb-95e0-2bed45a34176",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, num_mvts):\n",
    "  #print(\"mvts: \",i)\n",
    "  temp=trainData_scaled[i]\n",
    "  \n",
    "\n",
    "  temp_X = np.zeros( (num_params,8) )\n",
    "\n",
    "  for j in range(0, num_params):\n",
    "    #print(\"param: \",j)\n",
    "    ts = trainData_scaled[i,:, j]\n",
    "    #print(\"ts length: \",ts.shape)\n",
    "\n",
    "    v1 = np.mean(ts)\n",
    "    v2 = np.std(ts)\n",
    "    v3 = st.skew(ts)\n",
    "    v4 = st.kurtosis(ts)\n",
    "\n",
    "    ts_p = np.zeros((len(ts) - 1))\n",
    "    for k in range(len(ts_p)):\n",
    "       ts_p[k] = ts[k + 1] - ts[k]\n",
    "       \n",
    "    v5 = np.mean(ts_p)\n",
    "    v6 = np.std(ts_p)\n",
    "    v7 = st.skew(ts_p)\n",
    "    v8 = st.kurtosis(ts_p)\n",
    "    vect_ts = [v1, v2, v3, v4, v5, v6, v7, v8]\n",
    "    #X[i, j * 8:j * 8 + 8] = vect_ts\n",
    "    #print(col, \" vect_ts: \",vect_ts)\n",
    "    temp_X[j,:] = vect_ts\n",
    "    #print(\"x[\",col,\"]=\",X[col])\n",
    "  #print(\"temp_X: \",temp_X.shape)\n",
    "  #print(X)\n",
    "\n",
    "  row = temp_X.reshape((num_params*8))\n",
    "  x[i,:]=row\n",
    "\n",
    "  #x2=temp_X[0]\n",
    "  #for row in range(1, colNumber):\n",
    "    #x2=np.concatenate((x2,temp_X[1]), axis=0)\n",
    "  #print(\"x2.shape: \",x2.shape)\n",
    "  #x[table] = x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654b722b-27de-4912-b4fc-38482a064706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData_8n.shape:  (1540, 264)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.202742e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.577300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.984097e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.050571e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>7.335960e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1.415458e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.093650e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>5.326185e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>5.075406e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0    1.202742e-14\n",
       "1    1.000000e+00\n",
       "2    4.577300e-01\n",
       "3   -8.984097e-01\n",
       "4    3.050571e-02\n",
       "..            ...\n",
       "259  7.335960e+00\n",
       "260  1.415458e-02\n",
       "261  1.093650e+00\n",
       "262  5.326185e-01\n",
       "263  5.075406e+00\n",
       "\n",
       "[264 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData_8n=x\n",
    "#print(\"x.shape: \",x.shape) \n",
    "print(\"trainData_8n.shape: \",trainData_8n.shape)\n",
    "\n",
    "temp=trainData_8n[0]\n",
    "#print(temp)\n",
    "df = pd.DataFrame(temp)\n",
    "#df=pd.DataFrame.from_dict(trainData_8n)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e7ed36c-23be-4175-a166-80f4782ab204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def doBaseLineMethodsBasedCalcultions(inputData, X_train, X_test, y_train, y_test):\n",
    "  num_masterIteration=5\n",
    "\n",
    "  #inputData=trainData_8n\n",
    "\n",
    "  #print(type(inputData))\n",
    "  #print(type(inputData[0]))\n",
    "  #print(inputData.shape)\n",
    "\n",
    "  classification_report_dict=[]\n",
    "  #Accuracy=[][]\n",
    "  Accuracy=[]\n",
    "  roc_auc=[]\n",
    "  for masterIteration in range(num_masterIteration):\n",
    "      print(\"\\nmasterIteration: \",masterIteration)\n",
    "      #print(bcolors.WARNING + \"\\nmasterIteration :\" + bcolors.WARNING,masterIteration)\n",
    "      svm_model = SVC()\n",
    "      rf_model = RandomForestClassifier()\n",
    "      #forest = RandomForestClassifier(criterion='entropy',n_estimators=10, random_state=1, n_jobs=2)\n",
    "      nb_model = GaussianNB()\n",
    "      dt = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "      knn = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "      # all parameters not specified are set to their defaults\n",
    "      #logisticRegr = LogisticRegression()\n",
    "      #logisticRegr = LogisticRegression(max_iter=1000)\n",
    "      #https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "      logisticRegr = LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=4000)\n",
    "\n",
    "      #tree = DecisionTreeClassifier( random_state=23)\n",
    "      #adaboost = AdaBoostClassifier(base_estimator=tree, n_estimators=5, learning_rate=0.1, random_state=23)\n",
    "      #bagging = BaggingClassifier(base_estimator=tree, n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "      models = [svm_model, rf_model, nb_model,dt,knn]\n",
    "      #models = [logisticRegr]\n",
    "      for i in range(len(models)):\n",
    "          mod = models[i]\n",
    "          model_name = type(mod).__name__\n",
    "          #print( model_name)\n",
    "          mod.fit(X_train, y_train)\n",
    "\n",
    "          #print(f\"Train score: {mod.score(X_train, y_train)}\")\n",
    "          #print(f\"Test score: {mod.score(X_test, y_test)}\")\n",
    "\n",
    "          y_pred = mod.predict(X_test)\n",
    "          aaafsfsfsf=accuracy_score(y_test, y_pred)\n",
    "          #print(model_name, ' Accuracy: %.4f' %aaafsfsfsf )\n",
    "          Accuracy.append(aaafsfsfsf)\n",
    "\n",
    "          sfsgsg=roc_auc_score(y_test, y_pred)\n",
    "          roc_auc.append(sfsgsg)\n",
    "          #print(model_name, ' roc_auc: %.4f' %sfsgsg )\n",
    "          #print('Adjusted Accuracy : %.3f' % adjusted_rand_score(labels_true=y_test, labels_pred=y_pred))\n",
    "          #print(\"classification_report:\\n \", classification_report(y_test, y_pred))\n",
    "        \n",
    "\n",
    "          #TN, FP, FN, TP = \n",
    "          confusion_matrix=metrics.confusion_matrix(y_test, y_pred,labels=np.unique(trainLebel))\n",
    "          #print(model_name,'Confusion Matrix : \\n', confusion_matrix)\n",
    "          #print(type(y_test))\n",
    "          #print(type(y_pred))\n",
    "          sfsfsf2=metrics.classification_report(y_test, y_pred, digits=3)\n",
    "  \n",
    "          print(model_name,'classification_report : \\n',sfsfsf2)\n",
    "\n",
    "          sfsfsf=metrics.classification_report(y_test, y_pred, digits=3,output_dict=True)\n",
    "          classification_report_dict.append(sfsfsf)\n",
    "          \n",
    "          #print(model_name,'classification_report_dict : \\n',classification_report_dict)\n",
    "      #print('roc_auc : \\n',roc_auc)\n",
    "  doClassSpecificCalulcation(roc_auc,Accuracy,trainLebel,classification_report_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57ea7f6-57ed-44b7-bbd2-d96e4ca1b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doClassSpecificCalulcation(roc_auc,Accuracy,trainLebel,classification_report_dict):\n",
    "  print(\"\\n\\n\\n *************** Final Report ***************\")\n",
    "  print('\\np.mean(roc_auc) :',np.mean(roc_auc))\n",
    "  print('\\np.std(roc_auc) :',np.std(roc_auc))\n",
    "  print('\\np.mean(Accuracy) :',np.mean(Accuracy))\n",
    "  print('\\np.std(Accuracy) :',np.std(Accuracy))\n",
    "  print('\\n33333333 p.mean np.std(Accuracy) :     ',np.round(np.mean(Accuracy),2),\"+-\",np.std(Accuracy) )\n",
    "  for j in range( len(np.unique(trainLebel)) ):\n",
    "    print('\\n\\n\\n\\nclass :',j) \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1_score=[]\n",
    "    for i in range(len(classification_report_dict)):\n",
    "      report=classification_report_dict[i]\n",
    "      #print('classification_report : \\n',report) \n",
    "      temp=report[str(j)]['precision'] \n",
    "      precision.append(temp)\n",
    "\n",
    "      temp=report[str(j)]['recall'] \n",
    "      recall.append(temp)\n",
    "\n",
    "      temp=report[str(j)]['f1-score'] \n",
    "      f1_score.append(temp)\n",
    "\n",
    "    print('\\np.mean(precision) \\t p.mean(recall) \\t p.mean(f1_score) :') \n",
    "\n",
    "\n",
    "    print(np.mean(precision)) \n",
    "    print(np.mean(recall)) \n",
    "    print(np.mean(f1_score))\n",
    "\n",
    "    print('\\np.mean p.std(precision) \\tp.mean  p.std(recall) \\tp.mean  p.std(f1_score) :')\n",
    "\n",
    "    print(np.round(np.mean(precision),2),\"+-\",np.std(precision) )\n",
    "    print(np.round(np.mean(recall),2),\"+-\",np.std(recall) )\n",
    "    print(np.round(np.mean(f1_score),2),\"+-\",np.std(f1_score) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e57b613-1ecf-4138-a6df-60ca1029c7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1540, 60, 33)\n",
      "(1540, 33)\n",
      "(1540, 60, 33)\n",
      "(1540, 1980)\n"
     ]
    }
   ],
   "source": [
    "print(trainData_scaled.shape)\n",
    "n=len(trainData_scaled)\n",
    "lastRows=[]\n",
    "for l in range(0, n):\n",
    "  temp = trainData_scaled[l]\n",
    "  #print(temp.shape)\n",
    "  lastROw=temp[trainData_scaled.shape[1]-1]\n",
    "  #print(lastROw.shape)\n",
    "  #print(lastROw)\n",
    "  #print(type(lastROw))\n",
    "  lastRows.append(lastROw)\n",
    "\n",
    "#print(lastRows)\n",
    "df_lastRows = pd.DataFrame(lastRows)\n",
    "\n",
    "lastRows = np.array(lastRows)\n",
    "\n",
    "print(lastRows.shape)\n",
    "\n",
    "print(trainData_scaled.shape)\n",
    "n=len(trainData_scaled)\n",
    "flatten_tables=[]\n",
    "for l in range(0, n):\n",
    "  temp = trainData_scaled[l]\n",
    "  flatten_temp=temp.ravel()\n",
    "  #print(flatten_temp.shape)\n",
    "  #print(flatten_temp)\n",
    "  #print(type(flatten_temp))\n",
    "  flatten_tables.append(flatten_temp)\n",
    "\n",
    "df_flatten_tables = pd.DataFrame(flatten_tables)\n",
    "\n",
    "flatten_tables = np.array(flatten_tables)\n",
    "\n",
    "print(flatten_tables.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ba76bd8-ed5b-40e7-b5df-29cc12b1f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startCalculations(inputData):\n",
    "  #test_sizes=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "  test_sizes=[0.3]\n",
    "  for temp4 in range(len(test_sizes)):\n",
    "    test_size=test_sizes[temp4]\n",
    "    print(\"\\n\\n\\n *************** test_size: \",test_size)\n",
    "    random_state=0#random.randint(42, 100)\n",
    "    print(\"random_state: \",random_state)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputData, trainLebel, \n",
    "                                                            test_size=test_size, \n",
    "                                                            random_state=random_state)\n",
    "    print(\"X_train.shape X_test.shape y_train.shape y_test.shape \",\n",
    "              X_train.shape, X_test.shape ,y_train.shape, y_test.shape)\n",
    "    doBaseLineMethodsBasedCalcultions(inputData, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "186f5ae9-dd07-4df0-b37d-e46c05411b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " *************** test_size:  0.3\n",
      "random_state:  0\n",
      "X_train.shape X_test.shape y_train.shape y_test.shape  (1078, 33) (462, 33) (1078,) (462,)\n",
      "\n",
      "masterIteration:  0\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.689     0.732     0.710       239\n",
      "           1      0.692     0.646     0.668       223\n",
      "\n",
      "    accuracy                          0.690       462\n",
      "   macro avg      0.691     0.689     0.689       462\n",
      "weighted avg      0.691     0.690     0.690       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.687     0.762     0.722       239\n",
      "           1      0.711     0.628     0.667       223\n",
      "\n",
      "    accuracy                          0.697       462\n",
      "   macro avg      0.699     0.695     0.694       462\n",
      "weighted avg      0.698     0.697     0.695       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.634     0.297     0.405       239\n",
      "           1      0.520     0.816     0.635       223\n",
      "\n",
      "    accuracy                          0.548       462\n",
      "   macro avg      0.577     0.557     0.520       462\n",
      "weighted avg      0.579     0.548     0.516       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.611     0.632     0.621       239\n",
      "           1      0.591     0.570     0.580       223\n",
      "\n",
      "    accuracy                          0.602       462\n",
      "   macro avg      0.601     0.601     0.601       462\n",
      "weighted avg      0.601     0.602     0.601       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.820     0.708       239\n",
      "           1      0.707     0.466     0.562       223\n",
      "\n",
      "    accuracy                          0.649       462\n",
      "   macro avg      0.665     0.643     0.635       462\n",
      "weighted avg      0.663     0.649     0.637       462\n",
      "\n",
      "\n",
      "masterIteration:  1\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.689     0.732     0.710       239\n",
      "           1      0.692     0.646     0.668       223\n",
      "\n",
      "    accuracy                          0.690       462\n",
      "   macro avg      0.691     0.689     0.689       462\n",
      "weighted avg      0.691     0.690     0.690       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.691     0.803     0.743       239\n",
      "           1      0.745     0.614     0.673       223\n",
      "\n",
      "    accuracy                          0.712       462\n",
      "   macro avg      0.718     0.709     0.708       462\n",
      "weighted avg      0.717     0.712     0.709       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.634     0.297     0.405       239\n",
      "           1      0.520     0.816     0.635       223\n",
      "\n",
      "    accuracy                          0.548       462\n",
      "   macro avg      0.577     0.557     0.520       462\n",
      "weighted avg      0.579     0.548     0.516       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.611     0.632     0.621       239\n",
      "           1      0.591     0.570     0.580       223\n",
      "\n",
      "    accuracy                          0.602       462\n",
      "   macro avg      0.601     0.601     0.601       462\n",
      "weighted avg      0.601     0.602     0.601       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.820     0.708       239\n",
      "           1      0.707     0.466     0.562       223\n",
      "\n",
      "    accuracy                          0.649       462\n",
      "   macro avg      0.665     0.643     0.635       462\n",
      "weighted avg      0.663     0.649     0.637       462\n",
      "\n",
      "\n",
      "masterIteration:  2\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.689     0.732     0.710       239\n",
      "           1      0.692     0.646     0.668       223\n",
      "\n",
      "    accuracy                          0.690       462\n",
      "   macro avg      0.691     0.689     0.689       462\n",
      "weighted avg      0.691     0.690     0.690       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.664     0.778     0.717       239\n",
      "           1      0.709     0.578     0.637       223\n",
      "\n",
      "    accuracy                          0.682       462\n",
      "   macro avg      0.687     0.678     0.677       462\n",
      "weighted avg      0.686     0.682     0.678       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.634     0.297     0.405       239\n",
      "           1      0.520     0.816     0.635       223\n",
      "\n",
      "    accuracy                          0.548       462\n",
      "   macro avg      0.577     0.557     0.520       462\n",
      "weighted avg      0.579     0.548     0.516       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.611     0.632     0.621       239\n",
      "           1      0.591     0.570     0.580       223\n",
      "\n",
      "    accuracy                          0.602       462\n",
      "   macro avg      0.601     0.601     0.601       462\n",
      "weighted avg      0.601     0.602     0.601       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.820     0.708       239\n",
      "           1      0.707     0.466     0.562       223\n",
      "\n",
      "    accuracy                          0.649       462\n",
      "   macro avg      0.665     0.643     0.635       462\n",
      "weighted avg      0.663     0.649     0.637       462\n",
      "\n",
      "\n",
      "masterIteration:  3\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.689     0.732     0.710       239\n",
      "           1      0.692     0.646     0.668       223\n",
      "\n",
      "    accuracy                          0.690       462\n",
      "   macro avg      0.691     0.689     0.689       462\n",
      "weighted avg      0.691     0.690     0.690       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.721     0.745     0.733       239\n",
      "           1      0.716     0.691     0.703       223\n",
      "\n",
      "    accuracy                          0.719       462\n",
      "   macro avg      0.718     0.718     0.718       462\n",
      "weighted avg      0.719     0.719     0.718       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.634     0.297     0.405       239\n",
      "           1      0.520     0.816     0.635       223\n",
      "\n",
      "    accuracy                          0.548       462\n",
      "   macro avg      0.577     0.557     0.520       462\n",
      "weighted avg      0.579     0.548     0.516       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.611     0.632     0.621       239\n",
      "           1      0.591     0.570     0.580       223\n",
      "\n",
      "    accuracy                          0.602       462\n",
      "   macro avg      0.601     0.601     0.601       462\n",
      "weighted avg      0.601     0.602     0.601       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.820     0.708       239\n",
      "           1      0.707     0.466     0.562       223\n",
      "\n",
      "    accuracy                          0.649       462\n",
      "   macro avg      0.665     0.643     0.635       462\n",
      "weighted avg      0.663     0.649     0.637       462\n",
      "\n",
      "\n",
      "masterIteration:  4\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.689     0.732     0.710       239\n",
      "           1      0.692     0.646     0.668       223\n",
      "\n",
      "    accuracy                          0.690       462\n",
      "   macro avg      0.691     0.689     0.689       462\n",
      "weighted avg      0.691     0.690     0.690       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.696     0.787     0.739       239\n",
      "           1      0.734     0.632     0.680       223\n",
      "\n",
      "    accuracy                          0.712       462\n",
      "   macro avg      0.715     0.709     0.709       462\n",
      "weighted avg      0.715     0.712     0.710       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.634     0.297     0.405       239\n",
      "           1      0.520     0.816     0.635       223\n",
      "\n",
      "    accuracy                          0.548       462\n",
      "   macro avg      0.577     0.557     0.520       462\n",
      "weighted avg      0.579     0.548     0.516       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.611     0.632     0.621       239\n",
      "           1      0.591     0.570     0.580       223\n",
      "\n",
      "    accuracy                          0.602       462\n",
      "   macro avg      0.601     0.601     0.601       462\n",
      "weighted avg      0.601     0.602     0.601       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.820     0.708       239\n",
      "           1      0.707     0.466     0.562       223\n",
      "\n",
      "    accuracy                          0.649       462\n",
      "   macro avg      0.665     0.643     0.635       462\n",
      "weighted avg      0.663     0.649     0.637       462\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *************** Final Report ***************\n",
      "\n",
      "p.mean(roc_auc) : 0.6382524344709835\n",
      "\n",
      "p.std(roc_auc) : 0.05460342205731985\n",
      "\n",
      "p.mean(Accuracy) : 0.6387012987012988\n",
      "\n",
      "p.std(Accuracy) : 0.0582083530503633\n",
      "\n",
      "33333333 p.mean np.std(Accuracy) :      0.64 +- 0.0582083530503633\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class : 0\n",
      "\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.6496394295466803\n",
      "0.651213389121339\n",
      "0.6348134102460995\n",
      "\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.65 +- 0.03496339204232797\n",
      "0.65 +- 0.1879248261157996\n",
      "0.63 +- 0.12116519562166901\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class : 1\n",
      "\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.6466844877617995\n",
      "0.6252914798206278\n",
      "0.6234929416775471\n",
      "\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.65 +- 0.07870726924681459\n",
      "0.63 +- 0.11535794035463498\n",
      "0.62 +- 0.04604688782555611\n"
     ]
    }
   ],
   "source": [
    "startCalculations(lastRows)##(1540, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39938f34-4f30-49e3-8251-679e52762731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " *************** test_size:  0.3\n",
      "random_state:  0\n",
      "X_train.shape X_test.shape y_train.shape y_test.shape  (1078, 264) (462, 264) (1078,) (462,)\n",
      "\n",
      "masterIteration:  0\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.686     0.895     0.777       239\n",
      "           1      0.833     0.561     0.670       223\n",
      "\n",
      "    accuracy                          0.734       462\n",
      "   macro avg      0.760     0.728     0.724       462\n",
      "weighted avg      0.757     0.734     0.725       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.804     0.941     0.867       239\n",
      "           1      0.923     0.753     0.830       223\n",
      "\n",
      "    accuracy                          0.851       462\n",
      "   macro avg      0.863     0.847     0.848       462\n",
      "weighted avg      0.861     0.851     0.849       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.967     0.782       239\n",
      "           1      0.927     0.457     0.613       223\n",
      "\n",
      "    accuracy                          0.721       462\n",
      "   macro avg      0.792     0.712     0.697       462\n",
      "weighted avg      0.787     0.721     0.700       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.760     0.795     0.777       239\n",
      "           1      0.769     0.731     0.749       223\n",
      "\n",
      "    accuracy                          0.764       462\n",
      "   macro avg      0.764     0.763     0.763       462\n",
      "weighted avg      0.764     0.764     0.764       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.631     0.858     0.727       239\n",
      "           1      0.752     0.462     0.572       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.691     0.660     0.650       462\n",
      "weighted avg      0.689     0.667     0.652       462\n",
      "\n",
      "\n",
      "masterIteration:  1\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.686     0.895     0.777       239\n",
      "           1      0.833     0.561     0.670       223\n",
      "\n",
      "    accuracy                          0.734       462\n",
      "   macro avg      0.760     0.728     0.724       462\n",
      "weighted avg      0.757     0.734     0.725       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.820     0.912     0.863       239\n",
      "           1      0.893     0.785     0.835       223\n",
      "\n",
      "    accuracy                          0.851       462\n",
      "   macro avg      0.856     0.848     0.849       462\n",
      "weighted avg      0.855     0.851     0.850       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.967     0.782       239\n",
      "           1      0.927     0.457     0.613       223\n",
      "\n",
      "    accuracy                          0.721       462\n",
      "   macro avg      0.792     0.712     0.697       462\n",
      "weighted avg      0.787     0.721     0.700       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.760     0.795     0.777       239\n",
      "           1      0.769     0.731     0.749       223\n",
      "\n",
      "    accuracy                          0.764       462\n",
      "   macro avg      0.764     0.763     0.763       462\n",
      "weighted avg      0.764     0.764     0.764       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.631     0.858     0.727       239\n",
      "           1      0.752     0.462     0.572       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.691     0.660     0.650       462\n",
      "weighted avg      0.689     0.667     0.652       462\n",
      "\n",
      "\n",
      "masterIteration:  2\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.686     0.895     0.777       239\n",
      "           1      0.833     0.561     0.670       223\n",
      "\n",
      "    accuracy                          0.734       462\n",
      "   macro avg      0.760     0.728     0.724       462\n",
      "weighted avg      0.757     0.734     0.725       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.799     0.912     0.852       239\n",
      "           1      0.889     0.753     0.816       223\n",
      "\n",
      "    accuracy                          0.835       462\n",
      "   macro avg      0.844     0.833     0.834       462\n",
      "weighted avg      0.842     0.835     0.834       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.967     0.782       239\n",
      "           1      0.927     0.457     0.613       223\n",
      "\n",
      "    accuracy                          0.721       462\n",
      "   macro avg      0.792     0.712     0.697       462\n",
      "weighted avg      0.787     0.721     0.700       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.760     0.795     0.777       239\n",
      "           1      0.769     0.731     0.749       223\n",
      "\n",
      "    accuracy                          0.764       462\n",
      "   macro avg      0.764     0.763     0.763       462\n",
      "weighted avg      0.764     0.764     0.764       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.631     0.858     0.727       239\n",
      "           1      0.752     0.462     0.572       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.691     0.660     0.650       462\n",
      "weighted avg      0.689     0.667     0.652       462\n",
      "\n",
      "\n",
      "masterIteration:  3\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.686     0.895     0.777       239\n",
      "           1      0.833     0.561     0.670       223\n",
      "\n",
      "    accuracy                          0.734       462\n",
      "   macro avg      0.760     0.728     0.724       462\n",
      "weighted avg      0.757     0.734     0.725       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.807     0.925     0.862       239\n",
      "           1      0.904     0.762     0.827       223\n",
      "\n",
      "    accuracy                          0.846       462\n",
      "   macro avg      0.855     0.844     0.844       462\n",
      "weighted avg      0.854     0.846     0.845       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.967     0.782       239\n",
      "           1      0.927     0.457     0.613       223\n",
      "\n",
      "    accuracy                          0.721       462\n",
      "   macro avg      0.792     0.712     0.697       462\n",
      "weighted avg      0.787     0.721     0.700       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.760     0.795     0.777       239\n",
      "           1      0.769     0.731     0.749       223\n",
      "\n",
      "    accuracy                          0.764       462\n",
      "   macro avg      0.764     0.763     0.763       462\n",
      "weighted avg      0.764     0.764     0.764       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.631     0.858     0.727       239\n",
      "           1      0.752     0.462     0.572       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.691     0.660     0.650       462\n",
      "weighted avg      0.689     0.667     0.652       462\n",
      "\n",
      "\n",
      "masterIteration:  4\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.686     0.895     0.777       239\n",
      "           1      0.833     0.561     0.670       223\n",
      "\n",
      "    accuracy                          0.734       462\n",
      "   macro avg      0.760     0.728     0.724       462\n",
      "weighted avg      0.757     0.734     0.725       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.798     0.925     0.857       239\n",
      "           1      0.903     0.749     0.819       223\n",
      "\n",
      "    accuracy                          0.840       462\n",
      "   macro avg      0.850     0.837     0.838       462\n",
      "weighted avg      0.848     0.840     0.838       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.967     0.782       239\n",
      "           1      0.927     0.457     0.613       223\n",
      "\n",
      "    accuracy                          0.721       462\n",
      "   macro avg      0.792     0.712     0.697       462\n",
      "weighted avg      0.787     0.721     0.700       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.760     0.795     0.777       239\n",
      "           1      0.769     0.731     0.749       223\n",
      "\n",
      "    accuracy                          0.764       462\n",
      "   macro avg      0.764     0.763     0.763       462\n",
      "weighted avg      0.764     0.764     0.764       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.631     0.858     0.727       239\n",
      "           1      0.752     0.462     0.572       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.691     0.660     0.650       462\n",
      "weighted avg      0.689     0.667     0.652       462\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *************** Final Report ***************\n",
      "\n",
      "p.mean(roc_auc) : 0.7408957352196185\n",
      "\n",
      "p.std(roc_auc) : 0.06046392152807214\n",
      "\n",
      "p.mean(Accuracy) : 0.7459740259740261\n",
      "\n",
      "p.std(Accuracy) : 0.058586896400364394\n",
      "\n",
      "33333333 p.mean np.std(Accuracy) :      0.75 +- 0.058586896400364394\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class : 0\n",
      "\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.7076256684281502\n",
      "0.8875313807531381\n",
      "0.7845151113907234\n",
      "\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.71 +- 0.06533973815042723\n",
      "0.89 +- 0.05853339649831032\n",
      "0.78 +- 0.04282256430676586\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class : 1\n",
      "\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.8367309995975059\n",
      "0.5942600896860987\n",
      "0.6859548364176499\n",
      "\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.84 +- 0.06995914056459997\n",
      "0.59 +- 0.12952809035865703\n",
      "0.69 +- 0.09170787145232998\n"
     ]
    }
   ],
   "source": [
    "startCalculations(trainData_8n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dc556a5-aa7c-4b4c-9a5e-97fa15a49312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " *************** test_size:  0.3\n",
      "random_state:  0\n",
      "X_train.shape X_test.shape y_train.shape y_test.shape  (1078, 1980) (462, 1980) (1078,) (462,)\n",
      "\n",
      "masterIteration:  0\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.702     0.741     0.721       239\n",
      "           1      0.705     0.664     0.684       223\n",
      "\n",
      "    accuracy                          0.703       462\n",
      "   macro avg      0.704     0.702     0.702       462\n",
      "weighted avg      0.704     0.703     0.703       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.677     0.808     0.737       239\n",
      "           1      0.740     0.587     0.655       223\n",
      "\n",
      "    accuracy                          0.701       462\n",
      "   macro avg      0.709     0.697     0.696       462\n",
      "weighted avg      0.708     0.701     0.697       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.792     0.079     0.144       239\n",
      "           1      0.498     0.978     0.660       223\n",
      "\n",
      "    accuracy                          0.513       462\n",
      "   macro avg      0.645     0.529     0.402       462\n",
      "weighted avg      0.650     0.513     0.393       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.627     0.619     0.623       239\n",
      "           1      0.597     0.605     0.601       223\n",
      "\n",
      "    accuracy                          0.613       462\n",
      "   macro avg      0.612     0.612     0.612       462\n",
      "weighted avg      0.613     0.613     0.613       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.929     0.742       239\n",
      "           1      0.835     0.386     0.528       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.727     0.657     0.635       462\n",
      "weighted avg      0.723     0.667     0.639       462\n",
      "\n",
      "\n",
      "masterIteration:  1\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.702     0.741     0.721       239\n",
      "           1      0.705     0.664     0.684       223\n",
      "\n",
      "    accuracy                          0.703       462\n",
      "   macro avg      0.704     0.702     0.702       462\n",
      "weighted avg      0.704     0.703     0.703       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.684     0.833     0.751       239\n",
      "           1      0.766     0.587     0.665       223\n",
      "\n",
      "    accuracy                          0.714       462\n",
      "   macro avg      0.725     0.710     0.708       462\n",
      "weighted avg      0.724     0.714     0.709       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.792     0.079     0.144       239\n",
      "           1      0.498     0.978     0.660       223\n",
      "\n",
      "    accuracy                          0.513       462\n",
      "   macro avg      0.645     0.529     0.402       462\n",
      "weighted avg      0.650     0.513     0.393       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.627     0.619     0.623       239\n",
      "           1      0.597     0.605     0.601       223\n",
      "\n",
      "    accuracy                          0.613       462\n",
      "   macro avg      0.612     0.612     0.612       462\n",
      "weighted avg      0.613     0.613     0.613       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.929     0.742       239\n",
      "           1      0.835     0.386     0.528       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.727     0.657     0.635       462\n",
      "weighted avg      0.723     0.667     0.639       462\n",
      "\n",
      "\n",
      "masterIteration:  2\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.702     0.741     0.721       239\n",
      "           1      0.705     0.664     0.684       223\n",
      "\n",
      "    accuracy                          0.703       462\n",
      "   macro avg      0.704     0.702     0.702       462\n",
      "weighted avg      0.704     0.703     0.703       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.704     0.837     0.765       239\n",
      "           1      0.781     0.623     0.693       223\n",
      "\n",
      "    accuracy                          0.734       462\n",
      "   macro avg      0.743     0.730     0.729       462\n",
      "weighted avg      0.741     0.734     0.730       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.792     0.079     0.144       239\n",
      "           1      0.498     0.978     0.660       223\n",
      "\n",
      "    accuracy                          0.513       462\n",
      "   macro avg      0.645     0.529     0.402       462\n",
      "weighted avg      0.650     0.513     0.393       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.627     0.619     0.623       239\n",
      "           1      0.597     0.605     0.601       223\n",
      "\n",
      "    accuracy                          0.613       462\n",
      "   macro avg      0.612     0.612     0.612       462\n",
      "weighted avg      0.613     0.613     0.613       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.929     0.742       239\n",
      "           1      0.835     0.386     0.528       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.727     0.657     0.635       462\n",
      "weighted avg      0.723     0.667     0.639       462\n",
      "\n",
      "\n",
      "masterIteration:  3\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.702     0.741     0.721       239\n",
      "           1      0.705     0.664     0.684       223\n",
      "\n",
      "    accuracy                          0.703       462\n",
      "   macro avg      0.704     0.702     0.702       462\n",
      "weighted avg      0.704     0.703     0.703       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.663     0.808     0.728       239\n",
      "           1      0.731     0.561     0.635       223\n",
      "\n",
      "    accuracy                          0.688       462\n",
      "   macro avg      0.697     0.684     0.681       462\n",
      "weighted avg      0.696     0.688     0.683       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.792     0.079     0.144       239\n",
      "           1      0.498     0.978     0.660       223\n",
      "\n",
      "    accuracy                          0.513       462\n",
      "   macro avg      0.645     0.529     0.402       462\n",
      "weighted avg      0.650     0.513     0.393       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.627     0.619     0.623       239\n",
      "           1      0.597     0.605     0.601       223\n",
      "\n",
      "    accuracy                          0.613       462\n",
      "   macro avg      0.612     0.612     0.612       462\n",
      "weighted avg      0.613     0.613     0.613       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.929     0.742       239\n",
      "           1      0.835     0.386     0.528       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.727     0.657     0.635       462\n",
      "weighted avg      0.723     0.667     0.639       462\n",
      "\n",
      "\n",
      "masterIteration:  4\n",
      "SVC classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.702     0.741     0.721       239\n",
      "           1      0.705     0.664     0.684       223\n",
      "\n",
      "    accuracy                          0.703       462\n",
      "   macro avg      0.704     0.702     0.702       462\n",
      "weighted avg      0.704     0.703     0.703       462\n",
      "\n",
      "RandomForestClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.684     0.816     0.744       239\n",
      "           1      0.751     0.596     0.665       223\n",
      "\n",
      "    accuracy                          0.710       462\n",
      "   macro avg      0.718     0.706     0.705       462\n",
      "weighted avg      0.717     0.710     0.706       462\n",
      "\n",
      "GaussianNB classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.792     0.079     0.144       239\n",
      "           1      0.498     0.978     0.660       223\n",
      "\n",
      "    accuracy                          0.513       462\n",
      "   macro avg      0.645     0.529     0.402       462\n",
      "weighted avg      0.650     0.513     0.393       462\n",
      "\n",
      "DecisionTreeClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.627     0.619     0.623       239\n",
      "           1      0.597     0.605     0.601       223\n",
      "\n",
      "    accuracy                          0.613       462\n",
      "   macro avg      0.612     0.612     0.612       462\n",
      "weighted avg      0.613     0.613     0.613       462\n",
      "\n",
      "KNeighborsClassifier classification_report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.929     0.742       239\n",
      "           1      0.835     0.386     0.528       223\n",
      "\n",
      "    accuracy                          0.667       462\n",
      "   macro avg      0.727     0.657     0.635       462\n",
      "weighted avg      0.723     0.667     0.639       462\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *************** Final Report ***************\n",
      "\n",
      "p.mean(roc_auc) : 0.6411602904478676\n",
      "\n",
      "p.std(roc_auc) : 0.06613541709586601\n",
      "\n",
      "p.mean(Accuracy) : 0.6410389610389611\n",
      "\n",
      "p.std(Accuracy) : 0.07304527466713069\n",
      "\n",
      "33333333 p.mean np.std(Accuracy) :      0.64 +- 0.07304527466713069\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class : 0\n",
      "\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.6844184487933361\n",
      "0.6376569037656904\n",
      "0.5952186067440213\n",
      "\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.68 +- 0.06268550164724655\n",
      "0.64 +- 0.29688838742586415\n",
      "0.6 +- 0.22979482910107693\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class : 1\n",
      "\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.6777350906996281\n",
      "0.6446636771300449\n",
      "0.6269409873085878\n",
      "\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.68 +- 0.118690598309053\n",
      "0.64 +- 0.19136701815925405\n",
      "0.63 +- 0.05732041267378559\n"
     ]
    }
   ],
   "source": [
    "startCalculations(df_flatten_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e829e09-5632-43d8-90de-d3ce40816636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
