{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8WGAUGTitas",
    "outputId": "ff5d9f1f-96a9-4347-f644-946e6687e5f0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch #as th\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_name):\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        obj = pickle.load(fp)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/fuad/Documents/GitHub/models'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd() #'/Users/fuad/Documents/GitHub/models'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project folder location where the models and data folder exist\n",
    "projectlocation = '/Users/fuad/Documents/GitHub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3bGVKAljlex",
    "outputId": "0d702efe-7258-45a1-809c-56867a2f907e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVTS dataset shape:  (1540, 33, 60)   type:  <class 'numpy.ndarray'>\n",
      "Labels shape:  (1540,)   type:  <class 'numpy.ndarray'>\n",
      "labels_1540[0]:  <class 'numpy.int64'>\n",
      "unique labels:  [0 1 2 3]\n",
      "Example mvts:  [[6.74817790e+03 6.71324549e+03 6.71174659e+03 ... 6.95126400e+03\n",
      "  6.91616482e+03 6.95471100e+03]\n",
      " [8.29442125e+10 8.30111273e+10 8.32423665e+10 ... 8.90785513e+10\n",
      "  8.87904201e+10 8.89243169e+10]\n",
      " [1.59122525e+24 1.59118022e+24 1.58550781e+24 ... 1.59756967e+24\n",
      "  1.59518849e+24 1.58547757e+24]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 3.89738644e-05 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.42400000e+03 1.40000000e+03 1.40500000e+03 ... 2.23000000e+03\n",
      "  2.29700000e+03 2.18800000e+03]\n",
      " [3.85120000e-06 7.40970000e-06 2.92150000e-06 ... 2.30850000e-06\n",
      "  2.58090000e-06 6.63720000e-06]]\n"
     ]
    }
   ],
   "source": [
    "#set data path and load data\n",
    "datapath = projectlocation + \"/data/flare_prediction_mvts_data.pck\"\n",
    "labelpath = projectlocation + \"/data/flare_prediction_labels.pck\"\n",
    "\n",
    "mvts_1540=load(datapath)\n",
    "labels_1540=load(labelpath)\n",
    "\n",
    "#Check data\n",
    "print(\"MVTS dataset shape: \", mvts_1540.shape, \"  type: \", type(mvts_1540)) # (1540, 33, 60)\n",
    "print(\"Labels shape: \", labels_1540.shape, \"  type: \", type(labels_1540))     # (1540,)\n",
    "print(\"labels_1540[0]: \", type(labels_1540[0])) #<class 'numpy.int64'>\n",
    "print(\"unique labels: \", np.unique(labels_1540))# [0 1 2 3]\n",
    "print(\"Example mvts: \", mvts_1540[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary classification -->label conversion to BINARY class\n",
    "def get_binary_labels_from(labels_str):\n",
    "    tdf = pd.DataFrame(labels_str, columns = ['labels'])\n",
    "    data_classes= [0, 1, 2, 3]\n",
    "    d = dict(zip(data_classes, [0, 0, 1, 1])) \n",
    "    arr = tdf['labels'].map(d, na_action='ignore')\n",
    "    return arr.to_numpy()\n",
    "\n",
    "#labels_1540 = get_binary_labels_from(labels_1540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lIYuClryjrfM"
   },
   "outputs": [],
   "source": [
    "#Stratified train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mvts_1540, labels_1540, test_size=0.3, random_state=6, stratify=labels_1540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "form",
    "id": "qoXbnhcYc2oJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#Takes 3D array(x,y,z) >> transpose(y,z) >> return (x,z,y)\n",
    "def GetTransposed2D(arrayFrom):\n",
    "    toReturn = []\n",
    "    alen = arrayFrom.shape[0]\n",
    "    for i in range(0, alen):\n",
    "        toReturn.append(arrayFrom[i].T)\n",
    "    \n",
    "    return np.array(toReturn)\n",
    "\n",
    "#Takes 3D array(x,y,z) >> Flatten() >> return (y,z)\n",
    "def Make2D(array3D):\n",
    "    toReturn = []\n",
    "    x = array3D.shape[0]\n",
    "    y = array3D.shape[1]\n",
    "    for i in range(0, x):\n",
    "        for j in range(0, y):\n",
    "            toReturn.append(array3D[i,j])\n",
    "    \n",
    "    return np.array(toReturn)\n",
    "\n",
    "#Transform instance(92400, 33) into(1540x60x33)\n",
    "def Get3D_MVTS_from2D(array2D, windowSize):\n",
    "    arrlen = array2D.shape[0]\n",
    "    mvts = []\n",
    "    for i in range(0, arrlen, windowSize):\n",
    "        mvts.append(array2D[i:i+windowSize])\n",
    "\n",
    "    return np.array(mvts)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# normalize the dataset\n",
    "def GetStandardScaler(data2d):\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(data2d)\n",
    "    return scaler\n",
    "def GetStandardScaledData(data2d):\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(data2d)\n",
    "    #print(scaler.mean_)\n",
    "    data_scaled = scaler.transform(data2d)\n",
    "    return data_scaled\n",
    "\n",
    "def transform_scale_data(data3d, scaler):\n",
    "    #print(\"original data shape:\", data3d.shape) \n",
    "    trans = GetTransposed2D(data3d)\n",
    "    #print(\"transposed data shape:\", trans.shape)    #(x, 60, 33)\n",
    "    data2d = Make2D(trans)\n",
    "    #print(\"2d data shape:\", data2d.shape)    \n",
    "    #  scaler = GetStandardScaler(data2d)\n",
    "    data_scaled = scaler.transform(data2d)\n",
    "    mvts_scalled = Get3D_MVTS_from2D(data_scaled, data3d.shape[2])#,60)\n",
    "    #print(\"mvts data shape:\", mvts_scalled.shape)\n",
    "    transBack = GetTransposed2D(mvts_scalled)\n",
    "    #print(\"transBack data shape:\", transBack.shape)\n",
    "    return transBack\n",
    "\n",
    "def build_edge_index_tensor(adj):\n",
    "  num_nodes = adj.shape[0]\n",
    "  source_nodes_ids, target_nodes_ids = [], []\n",
    "  for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "      if(adj[i,j]==1):\n",
    "        source_nodes_ids.append(i)\n",
    "        target_nodes_ids.append(j)\n",
    "  edge_index = np.row_stack((source_nodes_ids, target_nodes_ids))\n",
    "  edge_index_tensor = torch.from_numpy(edge_index)\n",
    "  return edge_index_tensor\n",
    "\n",
    "def GetGraphAdjMtrx(squareMtx, thresolds, keep_weights=False): #Apply Thresolds to squareMtx\n",
    "    graphs = []\n",
    "    mtxLen = squareMtx.shape[0]\n",
    "    for thr in thresolds:\n",
    "        m = np.zeros((mtxLen,mtxLen))#r = []        \n",
    "        for i in range(0,mtxLen):\n",
    "            for j in range(0,mtxLen):\n",
    "                if i == j:# or squareMtx[i,j] > thr:\n",
    "                    m[i,j] = 1\n",
    "                elif squareMtx[i,j] > thr:\n",
    "                  if keep_weights == True:\n",
    "                    m[i,j] = squareMtx[i,j]\n",
    "                  else:\n",
    "                    m[i,j] = 1\n",
    "        graphs.append(m)#np.array(r))  \n",
    "    return graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THZJ7TNbdKvV",
    "outputId": "247865df-fdb3-4d5a-d400-4a8858ceb4f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data shape: (1078, 33, 60)\n",
      "transposed data shape: (1078, 60, 33)\n",
      "2d data shape: (64680, 33)\n",
      "mvts data shape: (1078, 60, 33)\n",
      "transBack data shape: (1078, 33, 60)\n",
      "original data shape: (462, 33, 60)\n",
      "transposed data shape: (462, 60, 33)\n",
      "2d data shape: (27720, 33)\n",
      "mvts data shape: (462, 60, 33)\n",
      "transBack data shape: (462, 33, 60)\n"
     ]
    }
   ],
   "source": [
    "#building standard scaler on train data X\n",
    "trans = GetTransposed2D(X_train)\n",
    "data2d = Make2D(trans)\n",
    "scaler = GetStandardScaler(data2d)\n",
    "\n",
    "X_train = transform_scale_data(X_train, scaler)\n",
    "X_test = transform_scale_data(X_test, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8sgVpKgkRwD",
    "outputId": "5e98195a-f60b-4d25-ff40-79641bf49316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1078, 33, 60)\n",
      "y_train shape:  (1078,)\n",
      "X_test shape:  (462, 33, 60)\n",
      "y_test shape:  (462,)\n",
      "y_train_counts\n",
      "{0: 270, 1: 269, 2: 269, 3: 270}\n",
      "y_test_counts\n",
      "{0: 115, 1: 116, 2: 116, 3: 115}\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "#check\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "unique_y_train, counts_y_train = np.unique(y_train, return_counts=True)\n",
    "y_train_stats = dict(zip(unique_y_train, counts_y_train))\n",
    "print(\"y_train_counts\")\n",
    "print(y_train_stats)\n",
    "#270/(269+269+270+270) = 0.25\n",
    "unique_y_test, counts_y_test = np.unique(y_test, return_counts=True)\n",
    "y_test_stats = dict(zip(unique_y_test, counts_y_test))\n",
    "print(\"y_test_counts\")\n",
    "print(y_test_stats)\n",
    "#116/(116+116+115+115) = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "s8H7kccGljXz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title graph utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_adj_mat(c, th=0, keep_weights=True):\n",
    "  #print(\"Creating graph with th: \", th)\n",
    "  n = c.shape[0]\n",
    "  a = np.zeros((n,n))\n",
    "  for i in range(n):\n",
    "    for j in range(n):\n",
    "      #print(\"before:\", c[i,j])\n",
    "      if(c[i,j]>th):\n",
    "        if(keep_weights):\n",
    "          a[i,j] = c[i,j]\n",
    "          a[j,i] = c[j,i]\n",
    "        else:\n",
    "          a[i,j] = 1\n",
    "          a[j,i] = 1\n",
    "      #print(\"after:\", a[i,j])\n",
    "  return a\n",
    "\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "def get_edge_index_weight_tensor(adj):\n",
    "  num_nodes = adj.shape[0]\n",
    "  source_nodes_ids, target_nodes_ids, edge_weights = [], [], []\n",
    "  for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "      if(adj[i,j]>0):\n",
    "        source_nodes_ids.append(i)\n",
    "        target_nodes_ids.append(j)\n",
    "        edge_weights.append(adj[i,j])\n",
    "  edge_index = np.row_stack((source_nodes_ids, target_nodes_ids))\n",
    "  edge_index_tensor = torch.from_numpy(edge_index)\n",
    "  edge_weights_np = np.asarray(edge_weights, dtype=np.float32)\n",
    "  edge_weights_tensor = torch.from_numpy(edge_weights_np)\n",
    "  #print(\"Index shape: \",edge_index_tensor.shape)\n",
    "  #print(\"Weight shape: \",edge_weights_tensor.shape)\n",
    "  #print(edge_index_tensor)\n",
    "  #print(edge_weights_tensor)\n",
    "  return edge_index_tensor, edge_weights_tensor\n",
    "\n",
    "def normalize_node_attributes(mvts):\n",
    "  sc = StandardScaler()\n",
    "  mvts_std = sc.fit_transform(mvts)\n",
    "  return mvts_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Qf70DFtZkTvw",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data crawler in train dataset\n",
    "num_temporal_split = 4\n",
    "th = 0\n",
    "num_train = X_train.shape[0]\n",
    "num_nodes = 25\n",
    "len_st = 15\n",
    "#populating adjacency matrices and node attributes of train events\n",
    "#(1078, 4, 6, 25, 25)\n",
    "train_adjs = np.zeros((num_train, num_temporal_split, num_nodes, num_nodes))\n",
    "train_nats = np.zeros((num_train, num_temporal_split, num_nodes, len_st))\n",
    "for i in range(num_train):\n",
    "  #print('Event: ', i)\n",
    "  mt = X_train[i].T[:,0:25] #consider first 25 solar params\n",
    "  #mt = normalize_node_attributes(mt) ++++++++++++++++++++++++++++++\n",
    "  for j in range(num_temporal_split):\n",
    "    #print('Temporal split: ', j*15, (j+1)*15)\n",
    "    smt = mt[j*15:(j+1)*15,:]#unnormalized\n",
    "    c_smt = np.corrcoef(smt.T)\n",
    "    c_smt[np.isnan(c_smt)]=0\n",
    "    for l in range(num_nodes): #gcnconv will automatically add self loops\n",
    "      c_smt[l,l] = 0\n",
    "    #smt = normalize_node_attributes(smt)\n",
    "    train_nats[i,j,:,:] = smt.T\n",
    "    adj = get_adj_mat(c_smt, th, True) #change from ex 10\n",
    "    #if(i==0 and j==0):\n",
    "      #print('train i, j: ', i, j)\n",
    "      #print(adj)\n",
    "      #print('is symetric: ', check_symmetric(adj))\n",
    "    train_adjs[i,j,:,:]=adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hUgLc_bfm3qF",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data crawler in test dataset\n",
    "num_test = X_test.shape[0]\n",
    "#populating adjacency matrices and node attributes of test events\n",
    "#(462, 4, 6, 25, 25)\n",
    "test_adjs = np.zeros((num_test, num_temporal_split, num_nodes, num_nodes))\n",
    "test_nats = np.zeros((num_test, num_temporal_split, num_nodes, len_st))\n",
    "for i in range(num_test):\n",
    "  #print('Test Event: ', i)\n",
    "  mt = X_test[i].T[:,0:25]\n",
    "  #mt = normalize_node_attributes(mt) +++++++++++++++++++++++++++++++++++++++++\n",
    "  for j in range(num_temporal_split):\n",
    "    #print('Temporal split: ', j*15, (j+1)*15)\n",
    "    smt = mt[j*15:(j+1)*15,:]\n",
    "    c_smt = np.corrcoef(smt.T)\n",
    "    c_smt[np.isnan(c_smt)]=0\n",
    "    for l in range(num_nodes): #gcnconv will automatically add self loops\n",
    "      c_smt[l,l] = 0\n",
    "    #smt = normalize_node_attributes(smt)\n",
    "    test_nats[i,j,:,:] = smt.T\n",
    "    adj = get_adj_mat(c_smt, th, True)\n",
    "    #if(i==0 and j==0):\n",
    "      #print('test i, j: ', i, j)\n",
    "      #print(adj)\n",
    "      #print('is symetric: ', check_symmetric(adj))\n",
    "    test_adjs[i,j,:,:]=adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-qyapvOlPA3",
    "outputId": "c5c213bf-d521-4b5f-e25c-ca13ddc05ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 4, 25, 25)\n",
      "(1078, 4, 25, 15)\n",
      "(462, 4, 25, 25)\n",
      "(462, 4, 25, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_adjs.shape)\n",
    "print(train_nats.shape)\n",
    "print(test_adjs.shape)\n",
    "print(test_nats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBpBgD_z8-XH"
   },
   "source": [
    "**Helper Methods to run epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "B_JrQ62F9L6g",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title RunEpochs get_accuracy trian test acc\n",
    "def RunEpochs(num_epochs = 1, print_loss_interval = 5): \n",
    "  for epoch in range(num_epochs):\n",
    "    for i in range(num_train):#num_train\n",
    "      model.zero_grad()\n",
    "\n",
    "      class_scores = model(train_adjs[i], train_nats[i]) \n",
    "      #target = [y_train[i]]\n",
    "      target = torch.from_numpy(np.array([y_train[i]]))\n",
    "      target = target.to(device)\n",
    "      loss = loss_function(class_scores, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    if(epoch % print_loss_interval == 0):\n",
    "      print (\"epoch n loss:\", epoch, loss)\n",
    "\n",
    "#------------------------------train acc\n",
    "def get_train_accuracy():\n",
    "  num_train = X_train.shape[0]\n",
    "  with torch.no_grad():\n",
    "    numCorrect = 0\n",
    "    for i in range(num_train):\n",
    "      train_class_scores = model(train_adjs[i], train_nats[i])\n",
    "      class_prediction = torch.argmax(train_class_scores, dim=-1) \n",
    "  \n",
    "      if(class_prediction == y_train[i]): \n",
    "        numCorrect = numCorrect + 1\n",
    "    return numCorrect/num_train\n",
    "\n",
    "\n",
    "#---------test acc\n",
    "def get_test_accuracy():\n",
    "  num_test = X_test.shape[0]\n",
    "  with torch.no_grad():\n",
    "    numCorrect = 0\n",
    "    for i in range(num_test):\n",
    "      test_class_scores = model(test_adjs[i], test_nats[i]) #(adj_mat_array, node_att_array)\n",
    "      class_prediction = torch.argmax(test_class_scores, dim=-1) \n",
    "      \n",
    "      if(class_prediction == y_test[i]): \n",
    "        numCorrect = numCorrect + 1\n",
    "    return numCorrect/num_test\n",
    "\n",
    "def get_accuracy():\n",
    "  print (\"train_accuracy:\", get_train_accuracy())\n",
    "  print (\"test_accuracy: \", get_test_accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Junw72qOScMg",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, adjusted_rand_score\n",
    "\n",
    "def get_accuracy_report_by_running_epochs(epochs, epoch_interval):\n",
    "  maxAcc=0\n",
    "  max_classification_report_dict=0\n",
    "  max_acc_epoch = 0\n",
    "  num_test = X_test.shape[0]\n",
    "\n",
    "  for epoch in range(epoch_interval, epochs, epoch_interval):\n",
    "    print(\"current epoch: \", epoch)\n",
    "    RunEpochs(num_epochs = epoch_interval, print_loss_interval = 300)\n",
    "    \n",
    "    #get_accuracy()\n",
    "    with torch.no_grad():\n",
    "      numCorrect = 0\n",
    "      predictaedLabel=[]\n",
    "      for i in range(num_test):\n",
    "        test_class_scores = model(test_adjs[i], test_nats[i]) #(adj_mat_array, node_att_array)\n",
    "        class_prediction = torch.argmax(test_class_scores, dim=-1) \n",
    "        predictaedLabel.append(class_prediction)\n",
    "        if(class_prediction == y_test[i]): \n",
    "          numCorrect = numCorrect + 1\n",
    "      acc = numCorrect/num_test\n",
    "      if acc  > maxAcc: #fgdg=round(acc, 2)\n",
    "        maxAcc=acc\n",
    "        max_acc_epoch = epoch\n",
    "        max_classification_report_dict=metrics.classification_report(y_test, predictaedLabel, digits=3,output_dict=True)\n",
    "\n",
    "  return maxAcc, max_acc_epoch, max_classification_report_dict   \n",
    "\n",
    "\n",
    "\n",
    "def doClassSpecificCalulcation(classification_report_dict, y_test):\n",
    "    \n",
    "  Accuracy = []\n",
    "  for i in range(len(classification_report_dict)):\n",
    "      report=classification_report_dict[i]\n",
    "      temp=report['accuracy']\n",
    "      Accuracy.append(temp)\n",
    "\n",
    "  print('mean(Accuracy) :',np.mean(Accuracy))\n",
    "  print('std(Accuracy) :',np.std(Accuracy))\n",
    "  print('mean np.std(Accuracy) : ',np.round(np.mean(Accuracy),2),\"+-\",np.round(np.std(Accuracy),2) )\n",
    "\n",
    "  for j in np.unique(y_test):#np.unique(trainLebel ): #. len(...) np.unique(trainLebel):  [0 1 2 3]\n",
    "    print('\\nclass :',j) \n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1_score=[]\n",
    "    for i in range(len(classification_report_dict)):\n",
    "      report=classification_report_dict[i]\n",
    "      #print('classification_report : \\n',report) \n",
    "      temp=report[str(j)]['precision'] \n",
    "      precision.append(temp)\n",
    "\n",
    "      temp=report[str(j)]['recall'] \n",
    "      recall.append(temp)\n",
    "\n",
    "      temp=report[str(j)]['f1-score'] \n",
    "      f1_score.append(temp)\n",
    "\n",
    "    print('p.mean(precision) \\t p.mean(recall) \\t p.mean(f1_score) :') \n",
    "    print(np.mean(precision), \"\\t\", np.mean(recall), \"\\t\", np.mean(f1_score)) \n",
    "    #print(np.mean(recall)) \n",
    "    #print(np.mean(f1_score))\n",
    "\n",
    "    print('p.mean p.std(precision) \\tp.mean  p.std(recall) \\tp.mean  p.std(f1_score) :')\n",
    "    print(np.round(np.mean(precision),2),\"+-\",np.round(np.std(precision),2) )\n",
    "    print(np.round(np.mean(recall),2),\"+-\",np.round(np.std(recall),2) )\n",
    "    print(np.round(np.mean(f1_score),2),\"+-\",np.round(np.std(f1_score),2) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rocket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sktime\n",
    "# OR\n",
    "#!pip install 'sktime[all_extras]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numba \n",
    "#-----> ROCKET compiles (via Numba) on import, which may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sktime.transformations.panel.rocket import Rocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rocket_predictions(Xtrain, ytrain, Xtest):\n",
    "    rocket = Rocket()\n",
    "    rocket.fit(Xtrain)\n",
    "    Xtrain_transform = rocket.transform(Xtrain)\n",
    "    classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "    classifier.fit(Xtrain_transform, ytrain)\n",
    "    \n",
    "    Xtest_transform = rocket.transform(Xtest)\n",
    "    preds = classifier.predict(Xtest_transform)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiment of 5 differeent random_state of train_test_data_split\n",
    "# experiment with different train size (0.1 to 0.9); change test_size\n",
    "\n",
    "classification_report_dict=[]\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(\"experiment running with random_state = \", i, \" ...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        mvts_1540, labels_1540, test_size=0.3, random_state = i, stratify=labels_1540)\n",
    "\n",
    "    trans = GetTransposed2D(X_train)\n",
    "    data2d = Make2D(trans)\n",
    "    scaler = GetStandardScaler(data2d)\n",
    "\n",
    "    X_train = transform_scale_data(X_train, scaler)\n",
    "    X_test = transform_scale_data(X_test, scaler)\n",
    "\n",
    "    predictions = get_rocket_predictions(X_train, y_train, X_test)\n",
    "    report_dict = metrics.classification_report(y_test, predictions, digits=3,output_dict=True)\n",
    "    classification_report_dict.append(report_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(Accuracy) : 0.7554112554112554\n",
      "std(Accuracy) : 0.025390265452147474\n",
      "mean np.std(Accuracy) :  0.76 +- 0.03\n",
      "\n",
      "class : 0\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.9169473873617037 \t 0.9878860569715142 \t 0.9510141644879433\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.92 +- 0.02\n",
      "0.99 +- 0.01\n",
      "0.95 +- 0.01\n",
      "\n",
      "class : 1\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.681855744050992 \t 0.7093553223388306 \t 0.6944818081886577\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.68 +- 0.05\n",
      "0.71 +- 0.02\n",
      "0.69 +- 0.03\n",
      "\n",
      "class : 2\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.5920424194465145 \t 0.5737481259370314 \t 0.5822800761203022\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.59 +- 0.03\n",
      "0.57 +- 0.05\n",
      "0.58 +- 0.04\n",
      "\n",
      "class : 3\n",
      "p.mean(precision) \t p.mean(recall) \t p.mean(f1_score) :\n",
      "0.8260429519077246 \t 0.7500299850074963 \t 0.7859666018024075\n",
      "p.mean p.std(precision) \tp.mean  p.std(recall) \tp.mean  p.std(f1_score) :\n",
      "0.83 +- 0.05\n",
      "0.75 +- 0.05\n",
      "0.79 +- 0.05\n"
     ]
    }
   ],
   "source": [
    "doClassSpecificCalulcation(classification_report_dict, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mvts_gnn_rnn_3_expt_12.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
